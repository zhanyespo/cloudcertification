QuestionNumber,FinalQuestion,GeneratedAnswerOptions,CorrectAnswer,Explanation
Q1,You need to create a partitioned table in an Azure Synapse Analytics dedicated SQL pool. How should you complete the Transact-SQL statement?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q2,"You have a data model that you plan to implement in a data warehouse in Azure Synapse Analytics as shown in the following exhibit. All the dimension tables will be less than 2 GB after compression, and the fact table will be approximately 6 TB. The dimension tables will be relatively static with very few data inserts and updates. Which type of table should you use for each table?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q3,"A Data Engineer is given a set of 10,000 CSV documents stored in Azure Data Lake that hold rows of historic sales data that needs to be accessed in a one-off analysis process into an Azure Synapse Analytics. What is the preferred method to access the sales rows?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q4,You use Azure Data Factory to prepare data to be queried by Azure Synapse Analytics serverless SQL pools. Files are initially ingested into an Azure Data Lake Storage Gen2 account as 10 small JSON files. Each file contains the same data attributes and data from a subsidiary of your company. You need to move the files to a different folder and transform the data to meet the following requirements: c® Provide the fastest possible query times. c® Automatically infer the schema from the underlying files. How should you configure the Data Factory copy activity?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q5,You need to output files from Azure Data Factory. Which file format should you use for each type of output?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q6,A company is migrating three on-premises Microsoft SQL Server databases to Azure. The company would like to minimize the cost of running the service in Azure. They have analyzed the usage of the databases before migration as shown below: Database 1: Used predominantly during the first week of the month with heavy analytics and querying during working hours (8:00 am-6:00 pm). Database 2: Used throughout the month for querying although data is uploaded nightly «Database 3: Used by the data science team to train their machine learning models within R and Python. This will be updated to be used within Azure Databricks once the migration has taken place. The training of the models will be performed daily. The data volumes can be handled easily by Azure SQL Database. How should the Azure SQL Databases be implemented?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q7,You are designing the folder structure for an Azure Data Lake Storage Gen2 container. Users will query data by using a variety of services including Azure Databricks and Azure Synapse Analytics serverless SQL pools. The data will be secured by subject area. Most queries will include data from the current year or current month. Which folder structure should you recommend to support fast queries and simplified folder security?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q8,You have an enterprise-wide Azure Data Lake Storage Gen2 account. The data lake is accessible only through an Azure virtual network named VNET1. You are building a SQL pool in Azure Synapse that will use data from the data lake. Your company has a sales team. All the members of the sales team are in an Azure Active Directory group named Sales. POSIX controls are used to assign the Sales group access to the files in the data lake. You plan to load data to the SQL pool every hour. You need to ensure that the SQL pool can load the sales data from the data lake. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each area selection is worth one point.,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q9,You are planning the deployment of Azure Data Lake Storage Gen2. You have the following two reports that will access the data lake: © Report1: Reads three columns from a file that contains 50 columns. ©® Report2: Queries a single record based on a timestamp. You need to recommend in which format to store the data in the data lake to support the reports. The solution must minimize read times. What should you recommend for each report?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q10,"When using Custom .Net activity with Azure Batch in an Azure Data Factory, what is the best general approach to store secrets and credentials to be accessible by your code?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q11,"You have a table named SalesFact in an enterprise data warehouse in Azure Synapse Analytics. SalesFact contains sales data from the past 36 months and has the following characteristics: c® Is partitioned by month ©® Contains one billion rows > Has clustered columnstore index At the beginning of each month, you need to remove data from SalesFact that is older than 36 months as quickly as possible. Which three actions should you perform in sequence in a stored procedure?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q12,"You have an enterprise data warehouse in Azure “ . ALTER EXTERNAL TABLE [Ext] . [Items] Synapse Analytics. ADD [ItemID] int; Using PolyBase, you create an external table named","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q13,"You have a table in an Azure Synapse Analytics dedicated SQL pool. The table was created by using the following Transact-SQL statement. CREATE TABLE [dbo] . [DimEmployee] ( [EmployeeKey] [int] IDENTITY(1,1) NOT NULL, You need to alter the table to meet the following [EmployeeID] [int] NOT NULL, requirements: [FirstName] [varchar] (100) NOT NULL, c® Ensure that users can identify the current (LastName] [varchar] (100) NOT NULL, © Support creating an employee reporting é : ! [StreetAddress] [varchar] (500) NOT NULL, hierarchy for your entire company. [City] [varchar] (200) NOT NULL, c® Provide fast lookup of the managers’ [StateProvince] [varchar] (50) NOT NULL, attributes such as name and job title. [Portalcode] [varchar] (10) NOT NULL Which column should you add to the table? )","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q14,"You need to transfer large amounts of key-value data to Azure for further processing and analysis and are reviewing the range o data storage and transfer methods to find which one best suits your use case. Here are the key factors to consider: 1.You will be gathering roughly 25 TBs of key-value data a month to be batched in once on a monthly basis. 2.The source data is gathered from multiple loT sensors and stored on a local on-premise data drive. 3.You are managing the data transfer yourself, for a small university study, and have little experience in managing data flow. 4.Your current network bandwidth is 1 Gbps. 5.The data will need minimal reformatting before you upload and perform SQL queries in Azure Synapse Analytics. You have identified Azure blob storage as the best destination for the initial upload of raw data. What service should you use to batch the data into Azure storage, given the above criteria? . Azure Data Factory . Azure Data Box . Azure Storage REST APIs AzCopy DOm>r Pm Plo  15:59/3:02:47 ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q15,You have two Azure Storage accounts named Storage1 and Storage2. Each account holds one container and has the hierarchical namespace enabled. The system has files that contain data stored in the Apache Parquet format. You need to copy folders and files from Storage’ to Storage2 by using a Data Factory copy activity. The solution must meet the following requirements: c® No transformations must be performed. c® The original folder structure must be retained. © Minimize time required to perform the copy activity. How should you configure the copy activity?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q16,You plan to implement an Azure Data Lake Gen 2 storage account. You need to ensure that the data lake will remain available if a data center fails in the primary Azure region. The solution must minimize costs. Which type of replication should you use for the storage account?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q17,You have a SQL pool in Azure Synapse. You plan to load data from Azure Blob storage to a staging table. Approximately 1 million rows of data will be loaded daily. The table will be truncated before each daily load. You need to create the staging table. The solution must minimize how long it takes to load the data to the staging table. How should you configure the table?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q18,"You are currently managing a company's Azure storage account. A best practice is to ensure that the keys associated with the storage account are frequently rotated. Which of the following methods is recommended when working with regeneration of storage keys, especially when you have applications dependent on those keys for data storage?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q19,"You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns. FactPurchase will have 1 million rows of data added daily and will contain three years of data. Transact-SQL queries similar to the following query will be executed daily. SELECT - SupplierKey, StockltemKey, IsOrderFinalized, COUNT(*) FROM FactPurchase - WHERE DateKey >= 20210101 - AND DateKey <= 20210131 - GROUP By SupplierKey, StockItemKey, IsOrderFinalized Which table distribution will minimize query times?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q20,You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB. You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics. You need to prepare the files to ensure that the data copies quickly. Solution: You convert the files to compressed delimited text files. Does this meet the goal? ,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q21,"You are configuring data security settings for separate Azure SQL databases. Database A stores social security numbers, which you want to prevent any users or applications from viewing. The social security numbers appear in one column within a single table of Database","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q22,You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB. You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics. You need to prepare the files to ensure that the data copies quickly. Solution: You modify the files to ensure that each row is more than 1 MB. Does this meet the goal?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q23,You build a data warehouse in an Azure Synapse Analytics dedicated SQL pool. Analysts write a complex SELECT query that contains multiple JOIN and CASE statements to transform data for use in inventory reports. The inventory reports will use the data and additional WHERE parameters depending on the report. The reports will be produced once daily. You need to implement a solution to make the dataset available for the reports. The solution must minimize query times. What should you implement?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q24,"While configuring a data workflow, you are deciding how to load data into an Azure Synapse Analytics staging table. You would like the data to load into the staging table as quickly as possible. Which distribution type(s) would be optimal for this scenario? Round Robin Replicated Either Round Robin or Hash-Distributed Either Replicated or Hash-Distributed SOmPr ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q25,"You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1. You plan to create a database named DB1 in Pool1. You need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool. Which format should you use for the tables in DB1?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q26,You are planning a solution to aggregate streaming data that originates in Apache Kafka and is output to Azure Data Lake Storage Gen2. The developers who will implement the stream processing solution use Java. Which service should you recommend using to process the streaming data?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q27,"You store files in an Azure Data Lake Storage Gen2 container. The container has the storage policy shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. ‘Answer Area “rules”: [ “enabled”: true, “contosorule”, “daysAfterCreationGreaterThan”: 60 ) “baseBlob”: ( “tierToCool”: { “daysAfterModi ficationGreaterThan”: 30 , “filters”: { “blobTypes” “blockBlob” ‘The files are [answer choice] after 30 days: v 1, “prefixMatch”: [ “container1/contoso” ","A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q28,"You plan to use a U-SQL script within Azure Data Factory (ADF) to transform data in an Azure Storage blob container, then load the transformed data into an Azure Synapse Analytics table. You want to manage the entire data flow within a single schedule. Which choice below is the most reasonable option to correctly complete the task?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q29,"You are designing a financial transactions table in an Azure Synapse Analytics dedicated SQL pool. The table will have a clustered columnstore index and will include the following columns: c® TransactionType: 40 million rows per transaction type c> CustomerSegment: 4 million per customer segment © TransactionMonth: 65 million rows per month © AccountType: 500 million per account type. You have the following query requirements: c Analysts will most commonly analyze transactions for a given month. > Transactions analysis will typically summarize transactions by transaction type, customer segment, and/or account type You need to recommend a partition strategy for the table to minimize query times. On which column should you recommend partitioning the table?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q30,You have an Azure Data Lake Storage Gen2 account named account! that stores logs as shown in the following table. Type | Designated retention period You do not expect that the logs will be accessed during the retention Application [360 days periods. Infrastructure [60 days You need to recommend a solution for account! that meets the following requirements: c® Automatically deletes the logs at the end of each retention period ~ Minimizes storage costs What should you include in the recommendation?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q31,"You plan to ingest streaming social media data by using Azure Stream Analytics. The data will be stored in files in Azure Data Lake Storage, and then consumed by using Azure Databricks and PolyBase in Azure Synapse Analytics. You need to recommend a Stream Analytics data output format to ensure that the queries from Databricks and PolyBase against the files encounter the fewest possible errors. The solution must ensure that the files can be queried quickly and that the data type information is retained. What should you recommend?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q32,You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool! contains a partitioned fact table named dbo.Sales and a staging table named stg.Sales that has the matching table and partition definitions. You need to overwrite the content of the first partition in dbo.Sales with the content of the same partition in stg.Sales. The solution must minimize load times. What should you do?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q33,"You are outlining the disaster recovery plan for your Azure SQL databases using active geo-replication. You plan to deploy your primary and secondary databases in different regions, to ensure greater availability in the event of a regional failure Both your primary and secondary databases’ firewalls should allow the same client IP address ranges, so that in the event the primary database fails the re-routed client-requests will successfully reach the secondary database. However, you do not want to allow more traffic than necessary to these or other databases in your production environment. What design choice below is recommended by Azure to ensure client requests are successfully received in the event of an unplanned failover?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q34,You have a Microsoft SQL Server database that uses a third normal form schema. You plan to migrate the data in the database to a star schema in an Azure Synapse Analytics dedicated SQL pool. You need to design the dimension tables. The solution must optimize read operations. What should you include in the solution?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q35,You plan to develop a dataset named Purchases by using Azure Databricks. Purchases will contain the following columns: c® ProductID c® ItemPrice ce LineTotal c® Quantity > StorelD c® Minute c® Month ce Hour ce Year - c® Day You need to store the data to support hourly incremental load pipelines that will vary for each Store ID. The solution must minimize storage costs. How should you complete the code?,"A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q36,"You are designing a partition strategy for a fact table in an Azure Synapse Analytics dedicated SQL pool. The table has the following specifications: c® Contain sales data for 20,000 products. Use hash distribution on a column named ProductID. c® Contain 2.4 billion records for the years 2019 and 2020. Which number of partition ranges provides optimal compression and performance for the clustered columnstore index?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q37,"You are managing a large relational database on SQL Server on Azure VM that is running SQL Server version 2017. This database needs to be joined with an external Hadoop data source, and then you must query both data sources using T-SQL. You have decided to implement Polybase to accomplish this task, but your query attempts using Polybase generate an error. What could have caused the implementation to fail? The SQL Server on Azure VM database exceeds the maximum possible row size of 32 KB. SQL Server 2017 does not support Polybase queries of Hadoop sources with T-SQL. Data should be migrated from the external Hadoop source to the database before using Polybase.","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q38,"You are creating dimensions for a data warehouse in an Azure Synapse Analytics dedicated SQL pool. You create a table by using the Transact-SQL statement shown in the following exhibit. CREATE TABLE [DBO] . [DimProduct] ( Use the drop-down menus to select the answer choice that eee a fieer aon ead NOT NULL, oduct Source ini 7 completes each statement based on the information presented in [ProductName] [nvarchar] (100) NOT NULL, the graphic. [ProductNumber] [nvarchar] (25) NOT NULL, NOTE: Each correct selection is worth one point. [Color] [nvarchar] (15) NULL, [Size] [nvarchar] (5) NULL, (Weight] [decimal] (8, 2) NULL, (ProductCategory] [nvarchar] (100) NULL, [SellStartDate] [date] NOT NULL, [SellEndDate] [date] NULL, [RowInsertedDateTime] [datetime] NOT NULL, [RowUpdatedDateTime] [datetime] NOT NULL, (ETLAuditID] [int] NOT NULL Answer Area DimProduct is @ fanswer choice] slowly changing dimension (SCD). v ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q39,"You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns. FactPurchase will have 1 million rows of data added daily and will contain three years of data. Transact-SQL queries similar to the following query will be executed daily. SELECT - SupplierKey, StockltemKey, COUNT(*) FROM FactPurchase - WHERE DateKey >= 20210101 - AND DateKey <= 20210131 - GROUP By SupplierKey, StockltemKey Which table distribution will minimize query times?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q40,You need to build a solution to ensure that users can query specific files in an Azure Data Lake Storage Gen2 account from an Azure Synapse Analytics serverless SQL pool. Which three actions should you perform in sequence?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q41,"You are configuring an Azure SQL database with geo-replication enabled, and now want to configure Azure SQL Database auditing to monitor database events and ensure compliance. While the majority of databases on the Azure SQL database instance will focus on managing application data, one database will focus on processing environment logs as a way to maximize resource costs. The logs database's events will be very different from the other databases’ events. The primary issue is auditing event types or categories for a specific database that differ from the rest of the databases on the server. What is the most effective way to enable auditing for this Azure SQL Database?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q42,"You are designing a data mart for the human resources (HR) department at your company. The data mart will contain employee information and employee transactions. From a source system, you have a flat extract that has the following fields: c® EmployeelD ce FirstName ce LastName c® Recipient c® GrossAmount c® TransactionID c® GovernmentID c> NetAmountPaid c® TransactionDate You need to design a star schema data model in an Azure Synapse Analytics dedicated SQL pool for the data mart. Which two tables should you create? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point.","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q43,You are designing a dimension table for a data warehouse. The table will track the value of the dimension attributes over time and preserve the history of the data by adding new rows as the data changes. Which type of slowly changing dimension (SCD) should you use?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q44,"You have data stored in thousands of CSV files in Azure Data Lake Storage Gen2. Each file has a header row followed by a properly formatted carriage return (/ r) and line feed (/n). You are implementing a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics by using PolyBase. You need to skip the header row when you import the files into the data warehouse. Before building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics. Which three actions should you perform in sequence?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q45,You are building an Azure Synapse Analytics dedicated SQL pool that will contain a fact table for transactions from the first half of the year 2020. You need to ensure that the table meets the following requirements: c® Minimizes the processing time to delete data that is older than 10 years c® Minimizes the I/O for queries that use year-to-date values How should you complete the Transact-SQL statement?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q46,"You are designing a data table in Azure Table Storage for optimal performance when handling a large number of read requests and a much smaller number of write requests. As part of your design preparation, you have a list of the most common queries to expect for the table. Most queries will be point queries, with exact matches for the partition and row of the desired item. There will be row range scan queries, which include an exact match for the item partition and a partial match for the item row. Almost all queries include two key properties, a group ID, which corresponds to many items, and an item ID, which is specific to one item. With this information, which of the following design choices is optimal for your Azure Storage table? . Use the group ID as the partition key and item ID as the row key . Use the item ID as the partition key and the group ID as the row key . Concatenate the group ID and item ID into a composite partition key, and leave an empty string as the row key","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q47,"You are performing exploratory analysis of the bus fare data in an Azure Data Lake Storage Gen2 account by using an Azure Synapse Analytics serverless SQL pool. You execute the Transact-SQL query shown in the following exhibit. SELECT payment_type, SUM(fare_amount) AS fare_total FROM OPENROWSET ( BULK ‘csv/busfare/tripdata_2020*.csv’, DATA_SOURCE = ‘BusData’, FORMAT = ‘CSV’, PARSER_VERSION = ‘2.0’, FIRSTROW = 2 What do the query results include?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q48,"You use PySpark in Azure Databricks to parse the following JSON input. “persons”: [ . a { You need to output the data in the following tabular format. “name” :""Keith”, age” :30, “dogs”: [“Fido”, “Fluffy” “name” :”Donna”, wage” :46, How should you complete the PySpark code?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q49,"You are designing an application that will store petabytes of medical imaging data. When the data is first created, the data will be accessed frequently during the first week. After one month, the data must be accessible within 30 seconds, but files will be accessed infrequently. After one year, the data will be accessed infrequently but must be accessible within five minutes. You need to select a storage strategy for the data. The solution must minimize costs. Which storage tier should you use for each time frame?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q50,"A Data Engineer is designing a dimensional model data solution. Within this solution, slowly changing dimension (SCD) is used to effectively manage the change of dimension members. When there is a change detected in the source, the dimension table data should be overwritten with the latest value without any record of the previous value. Which SCD type will be most suitable here? Type 1 Type 2 Type 3 None of the listed types are suitable. SOm,r ","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q51,You have an Azure Synapse Analytics Apache Spark pool named Pool1. You plan to load JSON files from an Azure Data Lake Storage Gen2 container into the tables in Pool1. The structure and data types vary by file. You need to load the files into the tables. The solution must maintain the source data types. What should you do?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q52,"You have the following Azure Stream Analytics query. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Answer Area Statements Yes The query combines two streams of partitioned data. [e) The stream scheme key and count must match the output scheme. roy Providing 60 streaming units will optimize the performance of the query. O- oo 0 WITH stepi AS (SELECT * FROM inputi PARTITION BY StateID INTO 10), step2 AS (SELECT * FROM input2 PARTITION BY StateID INTO 10) SELECT * INTO output FROM stepi PARTITION BY StateID UNION SELECT * INTO output FROM step2 PARTITION BY StateID ","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q53,"You are building a database in an Azure Synapse Analytics serverless SQL pool. You have data stored in Parquet files in an Azure Data Lake Storege Gen2 container. Records are structured as shown in the following sample. { ""id"": 123, “address_housenumber"": ""19c"", “address_line"": ""Memory Lane"", “applicant1_name"" “applicant2_name"" The records contain two applicants at most. You need to build a table that includes only the address fields. How should you complete the Transact-SQL statement?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q54,"You have Azure Data Lake Storage which contains a very large amount of data. There are various pipelines triggered for analyzing the data that arrived that day, week, and month, and the ADLS store needs a data archival policy that meets the following requirements: +New data will be requested and updated thousands of times in the first 30 days. After 30 days, data will be accessed occasionally and should be available immediately. After 180 days data will be accessed very infrequently if at all. Which actions should be taken to meet these requirements in the most cost-effective way? (Choose 2 answers)","A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q55,You have an Azure Synapse Analytics dedicated SQL pool named Pool! and an Azure Data Lake Storage Gen2 account named Account1. You plan to access the files in Account by using an external table. You need to create a data source in Pool1 that you can reference when you create the external table. How should you complete the Transact-SQL statement?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q56,You have an Azure subscription that contains an Azure Blob Storage account named storage’ and an Azure Synapse Analytics dedicated SQL pool named Pool1. You need to store data in storage1. The data will be read by Pool1. The solution must meet the following requirements: Enable Pool to skip columns and rows that are unnecessary in a query. c® Automatically create column statistics. c> Minimize the size of files. Which type of file should you use?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q57,"You plan to create a table in an Azure Synapse Analytics dedicated SQL pool. Data in the table will be retained for five years. Once a year, data that is older than five years will be deleted. You need to ensure that the data is distributed evenly across partitions. The solution must minimize the amount of time required to delete old data. How should you complete the Transact-SQL statement?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q58,A company has to implement an application that would generate PDF files. The application would only need to store the PDF's and JSON metadata related to the PDF files. The PDF files would then be distributed over the web to various users. The PDF files could grow large in size. What Azure data and storage solutions are recommended for the application?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q59,You have an Azure Data Lake Storage Gen2 service. You need to design a data archiving solution that meets the following requirements: c® Data that is older than five years is accessed infrequently but must be available within one second when requested. ce Data that is older than seven years is NOT accessed. -® Costs must be minimized while maintaining the required availability. How should you manage the data?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q60,You plan to create an Azure Data Lake Storage Gen2 account. You need to recommend a storage solution that meets the following requirements: c® Provides the highest degree of data resiliency c® Ensures that content remains available for writes if a primary data center fails What should you include in the recommendation?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q61,"You need to implement a Type 3 slowly changing dimension (SCD) for product category data in an Azure Synapse Analytics dedicated SQL pool. You have a table that was created by using the following Transact-SQL statement. Which two columns should you add to the table? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A [EffectiveEndDate] [datetime] NULL, 8. [CurrentProductCategory] [nvarchar] (100) NOT NULL, c. [ProductCategory] [nvarchar] (100) NOT NULL, oD (EffectiveStartDate] [datetime] NOT NULL, E [OriginalProductCategory] [nvarchar] (100) NOT NULL, Correct Answer: BE CREATE TABLE [DBO] . [DimProduct] ( [ProductKey] [int] IDENTITY (1,1) NOT NULL, [ProductSourceID] [int] NOT NULL, [ProductNane] [nvarchar] (100) NOT NULL, [Color] [nvarchar] (15) NULL, [SellstartDate] [date] NOT NULL, [Sellend0ate] [date] NULL, [RowInsertedDateTime] [datetime] NOT NULL, [RowipdatedDateTine] [datetime] NOT NULL, (ETLAuditID] [int] NoT NULL ) ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q62,"You have created an Azure Synapse Analytics managed table backed by Parquet in Spark and query from a serverless SQL pool. The following command was used: CREATE TABLE mytestdbo.myparquettable(id int, name string, birthdate date) USING Parquet Then you add a row in the table with values as given. What will be the output of the following query which will be run after a few mins? SELECT id FROM mytestdbo.myparquettable WHERE birthdate = '01-01-2001' ERROR NULL","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q63,"You have an Azure subscription. You plan to build a data warehouse in an Azure Synapse Analytics dedicated SQL pool named pool! that will contain staging tables and a dimensional model. Pool! will contain the following tables. You need to design the table storage for pooll. The solution must meet the following requirements: c® Maximize the performance of data loading operations to Staging.WebSessions. c® Minimize query times for reporting queries against the dimensional model. Name ‘Number of rows Update frequency Description Common. Date 7,300 New rows * Contains one row per date Inserted yearly forthe last 20 years + Contains columns named Year, Month, Quarter, and |sWeekend Marketing WebSessions 4,500 500,000 Hourly inserts and updates | Fact table that contains counts of and updates sessions and page views, including foreign key values for date, channel, device, and medium Staging WebSessions '300,000 Hourly truncation and inserts Staging table for web session data, truncation and including descriptive fields for inserts channel, device, nd medium Which type of table distribution should you use for each table?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q64,You have an Azure Synapse Analytics dedicated SQL pool. You need to create a table named FactinternetSales that will be a large fact table in a dimensional model. FactinternetSales will contain 100 million rows and two columns named SalesAmount and OrderQuantity. Queries executed on FactInternetSales will aggregate the values in SalesAmount and OrderQuantity from the last year for a specific product. The solution must minimize the data size and query execution time. How should you complete the code?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q65,You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1. Table1 contains the following: ® One billion rows © A clustered columnstore index c® A hash-distributed column named Product Key c® A column named Sales Date that is of the date data type and cannot be null Thirty million rows will be added to Table1 each month. You need to partition Table1 based on the Sales Date column. The solution must optimize query performance and data loading. How often should you create a partition?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q66,You have an Azure Databricks workspace that contains a Delta Lake dimension table named Table1. Table1 is a Type 2 slowly changing dimension (SCD) table. You need to apply updates from a source table to Table1. Which Apache Spark SQL operation should you use?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q67,"The following JSON is an example of a Parquet datase ‘ on Azure Blob Storage. “referenceNane”: “<Azure Blob Storage linked service name>"", Which of the JSON dataset properties is configured } “GFL S TEATS WASTED incorrectly? “Schema”: [ < physical schema, optional, retrievable during authoring > ],, “typeProperties”: zureBlobStorageLocation”, “: “containername”, folder/subfolder,","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q68,You are designing an Azure Data Lake Storage solution that will transform raw JSON files for use in an analytical workload. You need to recommend a format for the transformed files. The solution must meet the following requirements: © Contain information about the data types of each column in the files. c® Support querying a subset of columns in the files. c> Support read-heavy analytical workloads. > Minimize the file size. What should you recommend?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q69,You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB. You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics. You need to prepare the files to ensure that the data copies quickly. Solution: You modify the files to ensure that each row is less than 1 MB. Does this meet the goal?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q70,You plan to create a dimension table in Azure Synapse Analytics that will be less than 1 GB. You need to create the table to meet the following requirements: c® Provide the fastest query time. c® Minimize data movement during queries. Which type of table should you use?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q71,"You have to choose a real-time stream processing solution in Azure that meets the following requirements: 1.The solution must support Event Hubs, loT Hub, Kafka, and HDFS as input data sources. 2.The solution must support custom code as an input data format. 3.The solution needs built-in support for temporal processing. 4.Cosmos DB should be a supported sink. Which is the most suitable solution?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q72,You are designing a dimension table in an Azure Synapse Analytics dedicated SQL pool. You need to create a surrogate key for the table. The solution must provide the fastest query performance. What should you use for the surrogate key?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q73,"An electronics company needs to migrate Azure Data Lake Storage from ADLS Gen1 to Gen2 for a data analysis solution. The following are the key properties defining the current setup of ADLS Gen1"" Data Organization with file and folder support *Ecosystem based on Azure Databricks 3.1 for big data analytics Encryption of data at rest using customer-managed key «Traffic accepted only from Specific VMs in an integrated Virtual Network VNet integration utilizes network service endpoint security for authentication to ADLS storage Which of the above properties requires refactoring or re-architecting to complete the migration successfully? (Choose 2 answers) Data Organization with file and folder support Ecosystem based on Azure Databricks 3.1 for big data analytics Traffic accepted only from Specific VMs in integrated Virtual Network VNet integration utilizes network service endpoint security for authentication to ADLS storage SORE Pe Plo 1:17:5973:02:47 ","A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q74,"You have an Azure Data Lake Storage Gen2 account that contains a container named container1. You have an Azure Synapse Analytics serverless SQL pool that contains a native external table named dbo.Table1. The source data for dbo.Table1 is stored in container1. The folder structure of container1 is shown in the following exhibit. The external data source is defined by a using the following statement. CREATE EXTERNAL DATA SOURCE DataLake same (LOCATION = ‘https: //mydatalake.dfs.core.windows.net/containert/foldert/**"" » CREDENTIAL = DataLakeCred % For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Answer Area Statements Yes No When selecting all the rows in dbo.Table1, data from the mydata2.csv file will be returned. o 9° ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q75,You have an Azure Synapse Analytics dedicated SQL pool. You need to create a fact table named Table’ that will store sales data from the last three years. The solution must be optimized for the following query operations: + Show order counts by week. + Calculate sales totals by region. + Calculate sales totals by product. + Find all the orders from a given month. Which data should you use to partition Table1?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q76,You are designing the folder structure for an Azure Data Lake Storage Gen2 account. You identify the following usage patterns: + Users will query data by using Azure Synapse Analytics serverless SQL pools and Azure Synapse Analytics serverless Apache Spark pools. + Most queries will include a filter on the current year or week. + Data will be secured by data source. You need to recommend a folder structure that meets the following requirements: + Supports the usage patterns + Simplifies folder security + Minimizes query times Which folder structure should you recommend?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q77,The data team is designing a data analysis solution that includes several interdependent Azure Data Factory (ADF) pipelines. The first pipeline in a series will output values stored in Azure Synapse Analytics. Subsequent pipelines will need to reference the output value from the first pipeline and then proceed to an If Condition activity. What activity must the data team include in ADF pipelines to reference the external value stored in ADF and then proceed to the If Condition activity based on that value? Wait Activity ForEach Activity Set Variable Lookup Activity SORFr ,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q78,You have an Azure subscription that contains an Azure Synapse — Analytics dedicated SQL pool. (CREATE HOLE (dbo) .{Sales) You plan to deploy a solution that will analyze sales data and include ‘ ensue) sine sor wee the following: Icustomeetd] int NOT WILL country) Ant WoT WL + Atable named Country that will contain 195 rows + (Total) money MOF MLL + A table named Sales that will contain 100 million rows : + A query to identify total sales by country and customer from the = past 30 days rsrareution = ad HASH@{Customertd) ; _ HasH(OrderDate) You need to create the tables. The solution must maximize query REPLICATE performance. — cuustene coumnsrone 00% How should you complete the script?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q79,You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named account! and an Azure Synapse Analytics workspace named workspace1. You need to create an external table in a serverless SQL pool in workspace’. The external table will reference CSV files stored in account1. The solution must maximize performance. How should you configure the external table?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q80,"You have an Azure Synapse Analytics serverless SQL epee pees pool that contains a database named db1. The data model ‘StateProvince 1 Month . . . we, Region Year for db1 is shown in the following exhibit. ont Calarter Use the drop-down menus to select the answer choice that completes each statement based on the information one presented in the exhibit. CustomeriD (Ga CustomerName [] eae a . CustomerType FactOrders NOTE: Each correct selection is worth one point. Geographykey Customerkey an StoreKey Dimstore PrecuctKary StoreKey OrderDateKey ent StorelD | OrderNumber ProductiD StoreName OrderLineNumber | -—) —productName sucreh Cd ProductLinelD Geograp! cI To convert the data model to a star schema, [answer choice). o ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q81,"The following shows a table in a dedicated SQL CED Pool in Azure Synapse Analytics: A user was given permission to access the table using the following statement: menber1 menber2 GRANT SELECT ON Membership(Member!D, Name, Phone, Amount) TO TestUser; After entering the data into tables, the newly created TestUser tuns the following query SELECT * FROM Membership; What is the expected output of this query?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q82,You have an Azure Databricks workspace and an Azure Data Lake Storage Gen2 account named storage1. New files are uploaded daily to storage1. You need to recommend a solution that configures storage’ as a structured streaming source. The solution must meet the following requirements: + Incrementally process new files as they are uploaded to storage. + Minimize implementation and maintenance effort. + Minimize the cost of processing millions of files. + Support schema inference and schema drift. Which should you include in the recommendation?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q83,You have an Azure subscription that contains the resources shown in the following table. storage! | Azure Blob storage Contains publicly accessible TSV files that account do NOT have a header row [st [serene nneees Contains a serverless SQL pool You need to read the TSV files by using ad-hoc queries and the OPENROWSET function. The solution must assign a name and override the inferred data type of each column. What should you include in the OPENROWSET function?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q84,You have an Azure Synapse Analytics dedicated SQL pool that contains a table named DimSalesPerson. DimSalesPerson contains the following columns: + RepSourcelD + SalesRepID + FirstName + LastName * StartDate + EndDate + Region You are developing an Azure Synapse Analytics pipeline that includes a mapping data flow named Dataflow1. Dataflow1 will read sales team data from an external source and use a Type 2 slowly changing dimension (SCD) when loading the data into DimSalesPerson. You need to update the last name of a salesperson in DimSalesPerson. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point.,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q85,"A data team is designing several data processing solutions with complex Azure Data Factory (ADF) pipelines. These pipelines are dependent upon the successful completion of HDInsight processing jobs that will be initiated simultaneously with the ADF pipelines. In order for the ADF pipeline to complete successfully, the HDInsight job output must be available. Which ADF control flow activity would work best when the pipeline must confirm and evaluate the HDInsight output before proceeding to the next task?","A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q86,You plan to use an Azure Data Lake Storage Gen2 account to implement a Data Lake development environment that meets the following requirements: + Read and write access to data must be maintained if an availability zone becomes unavailable. + Data that was last modified more than two years ago must be deleted automatically. + Costs must be minimized. What should you configure?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q87,You are developing an Azure Synapse Analytics pipeline that will include a mapping data flow named Dataflow1. Dataflow1 will read customer data from an external source and use a Type 1 slowly changing dimension (SCD) when loading the data into a table named DimCustomer in an Azure Synapse Analytics dedicated SQL pool. You need to ensure that Dataflow1 can perform the following tasks: + Detect whether the data of a given customer has changed in the DimCustomer table. + Perform an upsert to the DimCustomer table. Which type of transformation should you use for each task?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q88,A data team is designing a data processing solution with an Azure Data Factory (ADF) pipeline that must call a custom REST endpoint from a Data Factory pipeline. Which ADF control activity will do this? . Wait Activity ForEach Activity . Web Activity . Lookup Activity pom> ,"A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q89,You have an Azure Synapse Analytics serverless SQL pool. You have an Azure Data Lake Storage account named adls1 that contains a public container named container1. The container1 container contains a folder named folder1. You need to query the top 100 rows of all the CSV files in folder1. How should you complete the query?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q90,"You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1. You plan to create a database named DB1 in Pool1. You need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool. Which format should you use for the tables in DB1?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q91,You have an Azure Data Lake Storage Gen2 account named storage1. You plan to implement query acceleration for storage1. Which two file types support query acceleration? Each correct answer presents a complete solution. NOTE: Each correct selection is worth one point.,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q92,"As a data engineer managing a company's Azure workloads, you need to upload 22 TB of data in Azure Storage into an Azure Synapse dedicated SQL pool. The 22 TB of data is Hive data in Optimized Row Columnar (ORC) format. After starting the upload, Azure displayed Java out-of-memory errors. You are not required to upload all of the data at once. Which steps could you take to complete the upload without generating similar errors? Use compressed delimited text files Export only a subset of the columns Colocate your storage layer and your dedicated SQL pool None of these options will prevent an error. com> ","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q93,You have an Azure subscription that contains the resources shown in the following table. Name Type Description ‘StorageT ‘Azure Blob storage Contains publicly accessible JSON files account WSt ‘Azure Synapse Analyics | Contains a serverless SL pool workspace You need to read the files in storage1 by using ad-hoc queries and the OPENROWSET function. The solution must ensure that each rowset contains a single JSON record. To what should you set the FORMAT option of the OPENROWSET function?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q94,"You have an Azure subscription that contains the Azure Synapse Analytics workspaces shown in the following table. Each workspace must read and write data to datalake1. Each workspace contains an unused Apache Spark pool. You plan to configure each Spark pool to share catalog objects that reference datalake1. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Answer Area Statements Yes The shared catalog objects can be stored in Azure Database for MySQL. © For the Apache Hive Metastore of each workspace, you must configure © 4 linked service that uses user-password authentication. ‘The users of workspace1 must be assigned the Storage Blob Contributor © role for datalake1. Name Primary storage account workspaceT datalaKet workspace? datalake2 workspaces datalaket ° ° ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q95,An application running on a third-party device has been programmed to send a stream of events to an loT Hub within Azure. You have set up an Azure Stream Analytics streaming job to use this loT Hub as a source. Your analysis team would like the data to be made available so that a Power BI dashboard will be updated automatically. Which sink type should be used? . Azure Data Factory . Azure Cosmos DB - Power BI Dataflow . Power BI Dataset poOmEr ,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q97,"You have a data warehouse. You need to implement a slowly changing dimension (SCD) named Product that will include three columns named ProductName, ProductColor, and ProductSize. The solution must meet the following requirements: + Prevent changes to the values stored in ProductName. + Retain only the current and the last values in ProductSize. + Retain all the current and previous values in ProductColor. Which type of SCD should you implement for each column?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q98,You have an Azure subscription that contains an Azure Synapse Analytics workspace named ws1 and an Azure Cosmos DB database account named Cosmos1. Cosmos contains a container named container1 and ws! contains a serverless SQL pool. You need to ensure that you can query the data in container1 by using the serverless SQL pool. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point.,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q99,"One of the largest fintech companies wants to utilize Azure Synapse Analytics for their new product, and expect that customers’ Personal Identifiable Information (Pll) will be stored in the databases. In addition, company employees on the data team members will require access to the dedicated SQL Pool created in Azure Synapse Analytics. How can you make sure that the Pll involved cannot be viewed by the data team? . Use Transparent Data Encryption (TDE) Use Dynamic Data Masking (DDM) Create user accounts without administrator privileges for data team members . Assign the SQL Security Manager role to data team members comPr ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q100,"You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named account! and a user named User. In account1, you create a container named container’. In container1, you create a folder named folder. You need to ensure that User1 can list and read all the files in folder. The solution must use the principle of least privilege. How should you configure the permissions for each folder?","A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q101,"You have an Azure Data Factory pipeline named pipeline1. You need to execute pipeline1 at 2 AM every day. The solution must ensure that if the trigger for pipeline stops, the next pipeline execution will occur at 2 AM, following a restart of the trigger. Which type of trigger should you create?","A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q102,"You have an Azure data factory named adf1 that contains a pipeline named ExecProduct. ExecProduct contains a data flow named Product. The Product data flow contains the following transformations: 1. WeeklyData: A source that points to a CSV file in an Azure Data Lake Storage Gen2 account with 20 columns 2. ProductColumns: A select transformation that selects from WeeklyData six columns named ProductID, ProductDescr, ProductSubCategory, ProductCategory, ProductStatus, and ProductLastUpdated 3. ProductRows: An aggregate transformation 4. ProductList: A sink that outputs data to an Azure Synapse Analytics dedicated SQL pool The Aggregate settings for ProductRows are ‘Aggregate settings Optimize Inspect Data preview eee: — ae — —o Grouped by: Producti Add a} a Gi Open expression builder cot topeion configured as shown in the following exhibit. answer Area Statements There will be six columns in the output of ProductRows. Leaen more C3 ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q103,"You are an Azure SQL Database Software as a Service developer, and suddenly your app undergoes tremendous demand. You need to accommodate the growth so you add more databases (shards). How do you redistribute the data to the new databases without disrupting the data integrity?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q104,You manage an enterprise data warehouse in Azure Synapse Analytics. Users report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries. You need to monitor resource utilization to determine the source of the performance issues. Which metric should you monitor?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q105,You have an Azure Synapse Analytics serverless SQL pool. You have an Apache Parquet file that contains 10 columns. You need to query data from the file. The solution must return only two columns. How should you complete the query?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q106,You plan to create a real-time monitoring app that alerts users when a device travels more than 200 meters away from a designated location. You need to design an Azure Stream Analytics job to process the data for the planned app. The solution must minimize the amount of code developed and the number of technologies used. What should you include in the Stream Analytics job?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q107,"An electronics company utilizes Azure Data Lake Storage (ADLS) Generation 1 for Big Data Analytics. As part of the data analytics team, your new assignment is to plan and design the migration of ADLS Generation 1 to ADLS Generation 2. Only a small number of existing pipelines are connected to the current data lakes, but your team requires that the migration results in no downtime for any related applications and that the process requires minimal administration. Which of the following migration methods would best meet these requirements? Lift and Shift Incremental Copy Dual Pipeline Bidirectional Sync PHP ","A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q108,A company has a real-time data analysis solution that is hosted on Microsoft Azure. The solution uses Azure Event Hub to ingest data and an Azure Stream Analytics cloud job to analyze the data. The cloud job is configured to use 120 Streaming Units (SU). You need to optimize performance for the Azure Stream Analytics job. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point.,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q109,You need to trigger an Azure Data Factory pipeline when a file arrives in an Azure Data Lake Storage Gen2 container. Which resource provider should you enable?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q110,You plan to perform batch processing in Azure Databricks once daily. Which type of Databricks cluster should you use?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q111,"You are processing streaming data from vehicles that pass through a toll booth. You need to use Azure Stream Analytics to return the license plate, vehicle make, and hour the last vehicle passed during each 10- minute window. How should you complete the query?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q113,"You have an Azure Data Factory instance that contains two pipelines named Pipeline? and Pipeline2. Pipeline’ has the activities shown in the following exhibit. Stored procedure ‘Set variable = stored procedure -—» (2X) Set variablet . Pipeline2 has the activities shown in the following exhibit. Execute Pipeline 7 Set variable » Execute Pipelinet ™ * (.X) Set variablet You execute Pipeline2, and Stored procedure! in Pipeline’ fails. What is the status of the pipeline runs?","A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q114,You have the following table named Employees. Ben ‘Smith 2017-12-15 | Standard You need to calculate the employee_type value based on the hire_date value. How should you complete the Transact-SQL statement?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q115,"You want to modify user-defined functions using T-SQL. The functions do not require a specific user-defined type, and you have ALTER permission on the schema. However, your ALTER FUNCTION requests fail. What will be the best possible cause? You are trying to change a scalar-valued function to a table-valued function. You must have ALTER permission on both the schema and on the function. You need EXECUTE permission on the user-defined type. The request altered related dependent functions and stored procedures. 9:op> You can modify user-defined functions in SQL Server by using SQL Server Management Studio or Transact-SQL. Modifying user- defined functions as described below will not change the functions’ permissions, nor will it affect any dependent functions, stored procedures, or triggers. Changing a user-defined function requires ALTER permission on the function or on the schema, not both. If the function specifies a user-defined type, you need EXECUTE permission on the type, but in this scenario EXECUTE permission is not required. When you use the ALTER function there are certain limitations ALTER FUNCTION cannot be used to perform any of the following actions: + Change a scalar valued function to a table-valued function, or vice versa. * Change a e function to a multistatement function, or vice versa. ","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q116,"You have an Azure Synapse Analytics workspace named WS1. You have an Azure Data Lake Storage Gen2 container that contains JSON-formatted files in nei ‘2020-06-10""19:49:34.5592"", the following format. mente You need to use the serverless SQL pool in WS1 to read the files. How should you complete the Transact-SQL statement?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q117,You have an Apache Spark DataFrame named temperatures. A sample of the data is shown in the following table. You need to produce the following table by using a Spark SQL query. Year JAN FEB MAR: APR MAY 2019 23 41 52 76 92 2020 24 42 [49 78 91 2021 26 53 [34 79 95 How should you complete the query?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q118,"You have an Azure Data Factory that contains 10 pipelines. You need to label each pipeline with its main purpose of either ingest, transform, or load. The labels must be available for grouping and filtering when using the monitoring experience in Data Factory. What should you add to each pipeline?","A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q119,"A data engineer has two tables of structured data that needs to be stored in an Azure Synapse Analytics. The Sales table consists of 3 billion rows and 4 columns. The three main columns used for analysis are OrderDate, CustomerReference, and ProductID. Approximately 10 million rows will be added each day. The Products table will hold product information such as color, size, category, subcategory, list price, manufacturing cost, and supplier. Most of the analytics would require joins between the two tables. How should the distribution of the tables be set up within the SQL Synapse Analytics?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q120,"The following code segment is used to create an Azure Databricks cluster. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. ","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q121,You are designing a statistical analysis solution that will use custom proprietary Python functions on near real-time data from Azure Event Hubs. You need to recommend which Azure service to use to perform the statistical analysis. The solution must minimize latency. What should you recommend?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q122,You have an enterprise data warehouse in Azure Synapse Analytics that contains a table named FactOnlineSales. The table contains data from the start of 2009 to the end of 2012. You need to improve the performance of queries against FactOnlineSales by using table partitions. The solution must meet the following requirements: ~ Create four partitions based on the order date. c» Ensure that each partition contains all the orders placed during a given calendar year. How should you complete the T-SQL command?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q123,A Data Engineer is setting up an Azure Data Factory pipeline to move data from an on-premises Microsoft SQL Server Database to Azure Synapse Analytics. The source table will be loaded by an internal process between 1:00 am and 3:00 am. The loading of the data should be started at 3:30 am. The transfer should be performed in the most efficient manner. What objects types will be required to setup this Azure Data Factory?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q124,"You need to implement a Type 3 slowly changing dimension (SCD) for product category data in an Azure Synapse Analytics dedicated SQL pool. You have a table that was created by using the following Transact-SQL statement. Which two columns should you add to the table? Each correct CME Teepe teeol (besbeogvce) -t [ProductKey} [int] IDENTITY (1,1) NOT NULL, answer presents part of the solution. [ProductSourceID] [int] NOT NULL, NOTE: Each correct selection is worth one point. (ProductName) [nvarchar] (100) NULL, [color] [varchar] (15) NULL, [sellstartDate] [date] NOT NULL, [sellgndDate] [date] NULL, [RowInsertedDateTime] [datetime] NOT NULL, [RowUpdatedDateTime] [datetime] NOT NULL, [ETEAuditID] [int] NOT NULL )","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q125,You are designing an Azure Stream Analytics solution that will analyze Twitter data. You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once. Solution: You use a hopping window that uses a hop size of 5 seconds and a window size 10 seconds. Does this meet the goal? ,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q126,You are building an Azure Stream Analytics job to identify how much time a user spends interacting with a feature on a webpage. The job receives events based on user actions on the webpage. Each row of data represents an event. Each event has a type of either ‘start’ or ‘end’. You need to calculate the duration between start and end events. How should you complete the query?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q127,You are the data engineer for a very large e-commerce website with a global userbase. You have a data pipeline that gathers clickstream with Azure Event Hub and sends it to Azure Stream Analytics. You need to design a query that will: Aggregate the number of clicks into distinct periods of time «Divide the numbers based on the user region Count each click once and only once Which of the following functions should be used in your query?,"A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q128,"You are creating an Azure Data Factory data flow that will ingest data from a CSV file, cast columns to specified types of data, and insert the data into a table in an Azure Synapse Analytic dedicated SQL pool. The CSV file contains three columns named username, comment, and date. The data flow already contains the following: ce A source transformation. c® A Derived Column transformation to set the appropriate types of data. c® A sink transformation to land the data in the pool. You need to ensure that the data flow meets the following requirements: c® All valid rows must be written to the destination table. c® Truncation errors in the comment column must be avoided proactively. c® Any rows containing comment values that will cause truncation errors upon insert must be written to a file in blob storage. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point.","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q129,"You need to create an Azure Data Factory pipeline to process data for the following three departments at your company: Ecommerce, retail, and wholesale. The solution must ensure that data can also be processed for the entire company. How should you complete the Data Factory data flow script?","A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q130,You have an Azure Data Lake Storage Gen2 account that contains a JSON file for customers. The file contains two attributes named FirstName and LastName. You need to copy the data from the JSON file to an Azure Synapse Analytics table by using Azure Databricks. A new column must be created that concatenates the FirstName and LastName values. You create the following components: c® A destination table in Azure Synapse c An Azure Blob storage container c® A service principal Which five actions should you perform in sequence next in is Databricks notebook?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q131,Which of the following statements regarding Azure Stream Analytics’ event ordering feature to manage late- arrived data is incorrect?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q132,You build an Azure Data Factory pipeline to move data from an Azure Data Lake Storage Gen2 container to a database in an Azure Synapse Analytics dedicated SQL pool. Data in the container is stored in the following folder structure. Jin /{YYYY}/{MM}/{DD}/{HH}/{mm} The earliest folder is /in/2021/01/01/00/00. The latest folder is /in/2021/01/15/01/45. You need to configure a pipeline trigger to meet the following requirements: c® Existing data must be loaded. c® Data must be loaded every 30 minutes. c® Late-arriving data of up to two minutes must be included in the load for the time at which the data should have arrived. How should you configure the pipeline trigger?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q133,You are designing a near real-time dashboard solution that will visualize streaming data from remote sensors that connect to the internet. The streaming data must be aggregated to show the average value of each 10-second interval. The data will be discarded after being displayed in the dashboard. The solution will use Azure Stream Analytics and must meet the following requirements: c® Minimize latency from an Azure Event hub to the dashboard. c® Minimize the required storage. c® Minimize development effort. Answer Area What should you include in the solution?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q134,"You are configuring an Azure SQL database with geo-replication enabled, and now want to configure Azure SQL Database auditing to monitor database events and ensure compliance. The critical consideration is to choose an auditing strategy that will monitor the primary and secondary databases so that it can maintain compliance in the event of a failover. Which choice does Microsoft recomend to meet this auditing requirement?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q135,You have an Azure Storage account and a data warehouse in Azure Synapse Analytics in the UK South region. You need to copy blob data from the storage account to the data warehouse by using Azure Data Factory. The solution must meet the following requirements: c® Ensure that the data remains in the UK South region at all times. ©® Minimize administrative effort. Which type of integration runtime should you use?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q136,You have an Azure SQL database named Database! and two Azure event hubs named HubA and HubB. The data consumed from each source is shown in the following table. You need to implement Azure Stream Analytics to calculate the average fare per mile by driver. How should you configure the Stream Analytics input for each source?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q137,You have an Azure Stream Analytics job that receives clickstream data from an Azure event hub. You need to define a query in the Stream Analytics job. The query must meet the following requirements: c® Count the number of clicks within each 10-second window based on the country of a visitor. c® Ensure that each click is NOT counted more than once. How should you define the Query?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q138,You are building an Azure Analytics query that will receive input data from Azure loT Hub and write the results to Azure Blob storage. You need to calculate the difference in the number of readings per sensor per hour. How should you complete the query?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q139,You need to schedule an Azure Data Factory pipeline to execute when a new file arrives in an Azure Data Lake Storage Gen2 container. Which type of trigger should you use?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q140,You have two Azure Data Factory instances named ADFdev and ADFprod. ADFdev connects to an Azure DevOps Git repository. You publish changes from the main branch of the Git repository to ADFdev. You need to deploy the artifacts from ADFdev to ADFprod. What should you do first?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q141,You are developing a solution that will stream to Azure Stream Analytics. The solution will have both streaming data and reference data. Which input type should you use for the reference data?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q142,"You are designing an Azure Stream Analytics job to process incoming events from sensors in retail environments. You need to process the events to produce a running average of shopper counts during the previous 15 minutes, calculated at five-minute intervals. Which type of window should you use?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q143,"You are designing a monitoring solution for a fleet of 500 vehicles. Each vehicle has a GPS tracking device that sends data to an Azure event hub once per minute. You have a CSV file in an Azure Data Lake Storage Gen2 container. The file maintains the expected geographical area in which each vehicle should be. You need to ensure that when a GPS position is outside the expected area, a message is added to another event hub for processing within 30 seconds. The solution must minimize cost. What should you include in the solution?","A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q144,You are designing an Azure Databricks table. The table will ingest an average of 20 million streaming events per day. You need to persist the events in the table for use in incremental load pipeline jobs in Azure Databricks. The solution must minimize storage costs and incremental load times. What should you include in the solution?,"A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q145,"You have a self-hosted integration runtime in Azure Data Factory. The current status of the integration runtime has the following configurations: c® Status: Running c Type: Self-Hosted ‘Answer Area ce Version: 4.4.7292.1 > Running / Registered Node(s): 1/1 c® High Availability Enabled: False ce Linked Count: 0 © Queue Length: 0 c> Average Queue Duration. 0.00s If the X-M node becomes unavailable, all executed pipelines will: The integration runtime has the following node details: The number of concurrent jobs and the c> Name: X-M CPU usage indicate that the Concurrent c® Status: Running Jobs (Running/Limit) value should be: ce Version: 4.4.7292.1 c® Available Memory: 7697MB © CPU Utilization: 6% c> Network (In/Out): 1.21KBps/0.83KBps c® Concurrent Jobs (Running/Limit): 2/14 Dispatcher/Worker al Status: v fail until the node comes back online switch to another integration runtime exceed the CPU limit iv raised lowered left as is ","A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q146,You have an Azure Databricks workspace named workspace in the Standard pricing tier. You need to configure workspace] to support autoscaling all-purpose clusters. The solution must meet the following requirements: © Automatically scale down workers when the cluster is underutilized for three minutes. ©® Minimize the time it takes to scale to the maximum number of workers. © Minimize costs. What should you do first?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q147,"You are designing an Azure Stream Analytics solution that will analyze Twitter data. You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once. Solution: You use a tumbling window, and you set the window size to 10 seconds. Does this meet the goal?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q148,You are designing an Azure Stream Analytics solution that will analyze Twitter data. You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once. Solution: You use a session window that uses a timeout size of 10 seconds. Does this meet the goal?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q149,You use Azure Stream Analytics to receive data from Azure Event Hubs and to output the data to an Azure Blob Storage account. You need to output the count of records received from the last five minutes every minute. Which windowing function should you use?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q150,You configure version control for an Azure Data Factory t ¢ instance as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Answer Area ‘Azure Resource Manager (ARM) templates for the pipeline assets are stored in [answer choice] B Unked services {© Integration runtimes @  scurcecontt By © Steontoaton © AaM template [@ Parameterization template tor $ ones Global parameters secu © customer managed ey Git repository Gi repository ntrmationaszocated with your data factory CVCD best practices [ ® senting Disconnect Repository type ‘Azure DevOps Git ‘Azure DevOps Account CONTOSO Project name ata Repository name duh bateheth Collaboration branch main Publish branch adt publish Root folder 1 if main adf_publish Parameterization template ,"A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q151,You are designing an Azure Stream Analytics solution that receives instant messaging data from an Azure Event Hub. You need to ensure that the output from the Stream Analytics job counts the number of messages per time zone every 15 seconds. How should you complete the Stream Analytics query?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q152,You have an Azure Data Factory instance named ADF1 and two Azure Synapse Analytics workspaces named WS1 and WS2. ADF1 contains the following pipelines: c® P1: Uses a copy activity to copy data from a nonpartitioned table in a dedicated SQL pool of WS1 to an Azure Data Lake Storage Gen2 account -® P2: Uses a copy activity to copy data from text-delimited files in an Azure Data Lake Storage Gen2 account to a nonpartitioned table in a dedicated SQL pool of WS2 You need to configure P1 and P2 to maximize parallelism and performance. Which dataset settings should you configure for the copy activity if each pipeline?,"A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q153,"You have an Azure Storage account that generates 200,000 new files daily. The file names have a format of {YYYY}/{MM}/{DD}/{HH}/{CustomerID}.csv. You need to design an Azure Data Factory solution that will load new data from the storage account to an Azure Data Lake once hourly. The solution must minimize load times and costs. How should you configure the solution?","A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q154,You have the following Azure Data Factory pipelines: c® Ingest Data from System1 > Ingest Data from System2 co Populate Dimensions c® Populate Facts Ingest Data from System1 and Ingest Data from System2 have no dependencies. Populate Dimensions must execute after Ingest Data from System] and Ingest Data from System2. Populate Facts must execute after Populate Dimensions pipeline. All the pipelines must execute every eight hours. What should you do to schedule the pipelines for execution?,"A. Use Azure Data Factory, B. Use Azure Stream Analytics, C. Use Azure Functions, D. Use Azure Event Grid",A. Use Azure Data Factory,Azure Data Factory is the best tool for managing and automating data pipelines for transformation and ingestion.
Q155,"You are responsible for providing access to an Azure Data Lake Storage Gen2 account. Your user account has contributor access to the storage account, and you have the application ID and access key. You plan to use PolyBase to load data into an enterprise data warehouse in Azure Synapse Analytics. You need to configure PolyBase to connect the data warehouse to storage account. Which three components should you create in sequence?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q156,"You are monitoring an Azure Stream Analytics job by using metrics in Azure. You discover that during the last 12 hours, the average watermark delay is consistently greater than the configured late arrival tolerance. What is a possible cause of this behavior?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q157,You are building an Azure Stream Analytics job to retrieve game data. You need to ensure that the job returns the highest scoring record for each five-minute time interval of each game. How should you complete the Stream Analytics query?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q158,"You have an Azure Data Lake Storage account that contains a staging zone. You need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics. Solution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. Does this meet the goal? ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q159,"You plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads: c® A workload for data engineers who will use Python and SQL. ® A workload for jobs that will run notebooks that use Python, Scala, and SQL. © A workload that data scientists will use to perform ad hoc analysis in Scala and R. The enterprise architecture team at your company identifies the following standards for Databricks environments: c® The data engineers must share a cluster. ® The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster. > All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists. You need to create the Databricks clusters for the workloads. Solution: You create a High Concurrency cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs. Does this meet the goal? ","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q160,You are designing an Azure Databricks cluster that runs user-defined local processes. You need to recommend a cluster configuration that meets the following requirements: c® Minimize query latency. > Maximize the number of users that can run queries on the cluster at the same time. co Reduce overall costs without compromising other requirements. Which cluster type should you recommend?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q161,"You are building an Azure Data Factory solution to process data received from Azure Event Hubs, and then ingested into an Azure Data Lake Storage Gen2 container. The data will be ingested every five minutes from devices into JSON files. The files have the following naming pattern. /{deviceType}/in/{YYYY}/{MM}/{DD}/{HH}/{devicelD}_{YYYY}{MM}{DD}HH}{mm}.json You need to prepare the data for batch data processing so that there is one dataset per hour per deviceType. The solution must minimize read times. How should you configure the sink for the copy activity?","A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q162,"You are designing an Azure Data Lake Storage Gen2 structure for telemetry data from 25 million devices distributed across seven key geographical regions. Each minute, the devices will send a JSON payload of metrics to Azure Event Hubs. You need to recommend a folder structure for the data. The solution must meet the following requirements: c® Data engineers from each region must be able to build their own pipelines for the data of their respective region only. c® The data must be processed at least once every 15 minutes for inclusion in Azure Synapse Analytics serverless SQL pools. How should you recommend completing the structure?","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q163,You are implementing an Azure Stream Analytics solution to process event data from devices. The devices output events when there is a fault and emit a repeat of the event every five seconds until the fault is resolved. The devices output a heartbeat event every five seconds after a previous event if there are no faults present. A sample of the events is shown in the following table. DevicelD EventType EventTime 78cc5ht9-w357-684r- | HeartBeat 2020-12-01T19:00.000Z You need to calculate the uptime between the faults. w4fr-kr16h6p9874e How should you complete the Stream Analytics SQL ‘78ccSht9-w357-684r- | HeartBeat 2020-12-01T 19:05.000Z juery?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q164,You are creating a new notebook in Azure Databricks that will support R as the primary language but will also support Scala and SQL. Which switch should you use to switch between languages?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q165,You have an Azure Data Factory pipeline that performs an incremental load of source data to an Azure Data Lake Storage Gen2 account. Data to be loaded is identified by a column named LastUpdatedDate in the source table. You plan to execute the pipeline every four hours. You need to ensure that the pipeline execution meets the following requirements: ® Automatically retries the execution when the pipeline run fails due to concurrency or throttling limits. > Supports backfilling existing data in the table. Which type of trigger should you use?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q166,You are designing a solution that will copy Parquet files stored in an Azure Blob storage account to an Azure Data Lake Storage Gen2 account. The data will be loaded daily to the data lake and will use a folder structure of {Year}/{Month}/{Day}/. You need to design a daily Azure Data Factory data load to minimize the data transfer between the two accounts. Which two configurations should you include in the design? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q167,You plan to build a structured streaming solution in Azure Databricks. The solution will count new events in five- minute intervals and report only events that arrive during the interval. The output will be sent to a Delta Lake table. Which output mode should you use?,"A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q168,"You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1. You have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1. You plan to insert data from the files in container into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of Table1. You need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1. Solution: In an Azure Synapse Analytics pipeline, you use a data flow that contains a Derived Column transformation. Does this meet the goal? ","A. Use a dedicated SQL pool, B. Use a serverless SQL pool, C. Use Azure Data Lake, D. Use Azure Blob Storage",A. Use a dedicated SQL pool,Dedicated SQL pools in Azure Synapse Analytics provide optimized performance for large-scale analytical queries.
Q169,A company has decided to implement an Azure Databricks solution. Two teams in the company have different requirements. Team 1 needs to do an exploratory analysis of data in an Azure Data Lake (Gen 2) hierarchical file system. Team 2 needs to run a data processing job twice a day. The job takes about one hour to run. Which types of clusters should be used to minimize costs?,"A. Store in Parquet format, B. Store in JSON format, C. Store in CSV format, D. Store in TXT format",A. Store in Parquet format,Parquet format is best for optimized query performance and reduced storage costs in Azure Data Lake.
Q170,"A dedicated SQL pool containing user information is accidentally dropped while it is in paused state. Upon investigation, you find that no user-defined restore point was created before the SQL pool was dropped. As a result of this accident, what statement regarding a snapshot of the dedicated SQL pool is correct?","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
Q171,"Your company sends shared access signatures (SAS) to verified third parties for Azure storage access. To enhance security, you're considering additional options. Evaluate these security features. Which of these can be uniquely controlled through stored access policies? Limiting access to a specific IP address range Specifying when access via SAS token starts and ends Limiting access to specific Azure storage containers or objects Revoking issued SAS tokens POV>","A. Option 1, B. Option 2, C. Option 3, D. Option 4",A. Option 1,Placeholder answer based on limited context.
