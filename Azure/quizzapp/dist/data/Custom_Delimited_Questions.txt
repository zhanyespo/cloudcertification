Q1<>You need to create a partitioned table in an Azure Synapse Analytics dedicated SQL pool. How should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Values Answer Area CLUSTERED INDEX CREATE TABLE tablel Gj COtEATE. ID INTEGER, DISTRIBUTION coll VARCHAR (10) , PARTITION col2 VARCHAR (10) PARTITION FUNCTION ), (Eze PARTITION SCHEME ‘ = HASH(ID), (ID RANGE LEFT FOR VALUES (1, 1000000, 2000000))<>No explanation provided.|||Q2<>You have a data model that you plan to implement in a data warehouse in Azure Synapse Analytics as shown in the following exhibit. All the dimension tables will be less than 2 GB after compression, and the fact table will be approximately 6 TB. The dimension tables will be relatively static with very few data inserts and updates. Which type of table should you use for each table? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area im_customer: ir iEmployeetD vcEmployeeLastName vcEmployeeMName veEmployeefirstName dtEmployeeHireDate dtEmployeetevel <dtEmployeelastPromotion ‘Azure Synapse Analytics iDailyBookingsiD iCustomertD iTimelD iEmployeetD iltemiD iQuantityOrdered dExchangeRate iCountryofOrigin munitPrice iCustomeriD veCustomerName veCustomerAddress veCustomercity iTimelD iCalendarDay iCalendarWeek ‘calendarMonth veDayofWeek veDayofMonth veDayofYear ‘olidayindicato<>No options available<>No explanation provided.|||Q3<>A Data Engineer is given a set of 10,000 CSV documents stored in Azure Data Lake that hold rows of historic sales data that needs to be accessed in a one-off analysis process into an Azure Synapse Analytics. What is the preferred method to access the sales rows? A. Import the sales rows in the Azure SQL Database using Azure Data Factory B. Import the sales rows in the Azure SQL Database using SQL Integration Services . Access the sales rows using Polybase Access the sales rows using a linked service So<>No options available<>No explanation provided.|||Q4<>You use Azure Data Factory to prepare data to be queried by Azure Synapse Analytics serverless SQL pools. Files are initially ingested into an Azure Data Lake Storage Gen2 account as 10 small JSON files. Each file contains the same data attributes and data from a subsidiary of your company. You need to move the files to a different folder and transform the data to meet the following requirements: c® Provide the fastest possible query times. c® Automatically infer the schema from the underlying files. How should you configure the Data Factory copy activity? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Copy behavior: v Flatten hierarchy Merge files Preserve hierarchy Sink file type: vv 6 You need to design an Azure Synapse Analytics dedicated SQL pool that meets the following requirements: co Can return an employee record from a given point in time. c® Maintains the latest employee information. c® Minimizes query complexity. How should you model the employee data? A. as a temporal table B. as a SQL graph table C. as a degenerate dimension table D. as a Type 2 slowly changing dimension (SCD) table<>No options available<>No explanation provided.|||Q5<>You need to output files from Azure Data Factory. Which file format should you use for each type of output? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Columnar format: Vv Avro GZip Parquet TXT JSON with a timestamp: v<>No options available<>No explanation provided.|||Q6<>A company is migrating three on-premises Microsoft SQL Server databases to Azure. The company would like to minimize the cost of running the service in Azure. They have analyzed the usage of the databases before migration as shown below: Database 1: Used predominantly during the first week of the month with heavy analytics and querying during working hours (8:00 am-6:00 pm). Database 2: Used throughout the month for querying although data is uploaded nightly «Database 3: Used by the data science team to train their machine learning models within R and Python. This will be updated to be used within Azure Databricks once the migration has taken place. The training of the models will be performed daily. The data volumes can be handled easily by Azure SQL Database. How should the Azure SQL Databases be implemented? A. ach having a set number of DTUs. B. Database 3 as an Azure SQL Data Warehouse and databases 1 and 2 on an Azure SQL VM C. Convert database 3 to an Azure Data Lake (Gen 2) and databases 1 and 2 as Cosmos DB D. Implement all as Azure SQL Databases included in a single elastic pool<>No options available<>No explanation provided.|||Q7<>You are designing the folder structure for an Azure Data Lake Storage Gen2 container. Users will query data by using a variety of services including Azure Databricks and Azure Synapse Analytics serverless SQL pools. The data will be secured by subject area. Most queries will include data from the current year or current month. Which folder structure should you recommend to support fast queries and simplified folder security? A. /{SubjectArea}/{DataSource}/{DD}/{MM}/{YYYY}/{FileData}_{YYYY}_{MM}_{DD}.csv B. /{DD}/{MM}/{YYYY}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv C. YYYY}/{MM}/{DD}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv D. /{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv<>No options available<>No explanation provided.|||Q8<>You have an enterprise-wide Azure Data Lake Storage Gen2 account. The data lake is accessible only through an Azure virtual network named VNET1. You are building a SQL pool in Azure Synapse that will use data from the data lake. Your company has a sales team. All the members of the sales team are in an Azure Active Directory group named Sales. POSIX controls are used to assign the Sales group access to the files in the data lake. You plan to load data to the SQL pool every hour. You need to ensure that the SQL pool can load the sales data from the data lake. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each area selection is worth one point. A. Add the managed identity to the Sales group. B. Use the managed identity as the credentials for the data load process. C. Create a shared access signature (SAS). D. Add your Azure Active Directory (Azure AD) account to the Sales group. E. Use the shared access signature (SAS) as the credentials for the data load process. F. Create a managed identity.<>No options available<>No explanation provided.|||Q9<>You are planning the deployment of Azure Data Lake Storage Gen2. You have the following two reports that will access the data lake: © Report1: Reads three columns from a file that contains 50 columns. ©® Report2: Queries a single record based on a timestamp. You need to recommend in which format to store the data in the data lake to support the reports. The solution must minimize read times. What should you recommend for each report? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Report1: iv Avro CSV Parquet TsV Report2:<>No options available<>No explanation provided.|||Q10<>When using Custom .Net activity with Azure Batch in an Azure Data Factory, what is the best general approach to store secrets and credentials to be accessible by your code? A. B. ie} Hold any secrets and/or credentials in an Azure Key Vault, and use a certificate-based service principal to protect the key vault. Distribute the certificate to the Azure Batch pool. Create an Azure SQL linked service with connection string settings, and create a dataset that uses the linked service. Finally, chain the dataset as a dummy input dataset to the custom .NET activity. . Store the secrets/credentials directly in your app.config (configuration file) in plain text. . Store the secrets/credentials directly in your app.config (configuration file) in encrypted format, and use the decryption key at runtime to decrypt the secrets.<>No options available<>No explanation provided.|||Q11<>You have a table named SalesFact in an enterprise data warehouse in Azure Synapse Analytics. SalesFact contains sales data from the past 36 months and has the following characteristics: c® Is partitioned by month ©® Contains one billion rows > Has clustered columnstore index At the beginning of each month, you need to remove data from SalesFact that is older than 36 months as quickly as possible. Which three actions should you perform in sequence in a stored procedure? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Select and Place: Actions Answer Area Switch the partition containing the stale data from SalesFact to SalesFact_Work. Truncate the partition containing the stale data. Drop the SalesFact_Work table. Create an empty table named SalesFact_Work that has the same schema as SalesFact. Pm Plo 12:59/3:02:47<>No options available<>No explanation provided.|||Q12<>You have an enterprise data warehouse in Azure “ . ALTER EXTERNAL TABLE [Ext] . [Items] Synapse Analytics. ADD [ItemID] int; Using PolyBase, you create an external table named B. [Ext].[Items] to query Parquet files stored in Azure DROP EXTERNAL FILE FORMAT parquetfilel; Data Lake Storage Gen2 without importing the data a to the data warehouse. FORMAT_TYPE = PARQUET, . The éclemalltable has three columns. 17 BTA ScHRESTOR = ‘org.Apache.tadoop.Se.compress.AnEppyCOMe You discover that the Parquet files have a fourth c column named ItemID. DROP EXTERNAL TABLE [Ext] . [Items] Which command should you run to add the ItemID CREATE! areuuay Shere exe) tees) ({ZtemID] [int] NULL, (xtemName] nvarchar (50) NULL, [Itemtype] nvarchar (20) NULL, (1tembescription] nvarchar(250)) wiTe ( column to the external table? LOCATION= ‘/Items/", DATA_SOURCE = AzureDataLakeStore,<>No options available<>No explanation provided.|||Q13<>You have a table in an Azure Synapse Analytics dedicated SQL pool. The table was created by using the following Transact-SQL statement. CREATE TABLE [dbo] . [DimEmployee] ( [EmployeeKey] [int] IDENTITY(1,1) NOT NULL, You need to alter the table to meet the following [EmployeeID] [int] NOT NULL, requirements: [FirstName] [varchar] (100) NOT NULL, c® Ensure that users can identify the current (LastName] [varchar] (100) NOT NULL, © Support creating an employee reporting é : ! [StreetAddress] [varchar] (500) NOT NULL, hierarchy for your entire company. [City] [varchar] (200) NOT NULL, c® Provide fast lookup of the managers’ [StateProvince] [varchar] (50) NOT NULL, attributes such as name and job title. [Portalcode] [varchar] (10) NOT NULL Which column should you add to the table? ) A. [ManagerEmployeelD] [smallint] NULL B. [ManagerEmployeeKey] [smallint] NULL C. [ManagerEmployeeKey] [int] NULL D. [ManagerName] [varchar](200) NULL.<>No options available<>No explanation provided.|||Q14<>You need to transfer large amounts of key-value data to Azure for further processing and analysis and are reviewing the range o data storage and transfer methods to find which one best suits your use case. Here are the key factors to consider: 1.You will be gathering roughly 25 TBs of key-value data a month to be batched in once on a monthly basis. 2.The source data is gathered from multiple loT sensors and stored on a local on-premise data drive. 3.You are managing the data transfer yourself, for a small university study, and have little experience in managing data flow. 4.Your current network bandwidth is 1 Gbps. 5.The data will need minimal reformatting before you upload and perform SQL queries in Azure Synapse Analytics. You have identified Azure blob storage as the best destination for the initial upload of raw data. What service should you use to batch the data into Azure storage, given the above criteria? . Azure Data Factory . Azure Data Box . Azure Storage REST APIs AzCopy DOm>r Pm Plo  15:59/3:02:47<>No options available<>No explanation provided.|||Q15<>You have two Azure Storage accounts named Storage1 and Storage2. Each account holds one container and has the hierarchical namespace enabled. The system has files that contain data stored in the Apache Parquet format. You need to copy folders and files from Storage’ to Storage2 by using a Data Factory copy activity. The solution must meet the following requirements: c® No transformations must be performed. c® The original folder structure must be retained. © Minimize time required to perform the copy activity. How should you configure the copy activity? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Source dataset type: iv<>No options available<>No explanation provided.|||Q16<>You plan to implement an Azure Data Lake Gen 2 storage account. You need to ensure that the data lake will remain available if a data center fails in the primary Azure region. The solution must minimize costs. Which type of replication should you use for the storage account? A. geo-redundant storage (GRS) B. geo-zone-redundant storage (GZRS) C. locally-redundant storage (LRS) D. zone-redundant storage (ZRS)<>No options available<>No explanation provided.|||Q17<>You have a SQL pool in Azure Synapse. You plan to load data from Azure Blob storage to a staging table. Approximately 1 million rows of data will be loaded daily. The table will be truncated before each daily load. You need to create the staging table. The solution must minimize how long it takes to load the data to the staging table. How should you configure the table? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Distribution:<>No options available<>No explanation provided.|||Q18<>You are currently managing a company's Azure storage account. A best practice is to ensure that the keys associated with the storage account are frequently rotated. Which of the following methods is recommended when working with regeneration of storage keys, especially when you have applications dependent on those keys for data storage? A. Regenerate the primary key, and then make all applications using this key. B. Regenerate the secondary key and then make all applications using this key. C. Ensure all applications use the secondary key. Regenerate the primary key and move all applications back to the primary key. Regenerate the secondary key. D. Ensure all applications use the secondary key. Regenerate the primary key and secondary key.<>No options available<>No explanation provided.|||Q19<>You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns. FactPurchase will have 1 million rows of data added daily and will contain three years of data. Transact-SQL queries similar to the following query will be executed daily. SELECT - SupplierKey, StockltemKey, IsOrderFinalized, COUNT(*) FROM FactPurchase - WHERE DateKey >= 20210101 - AND DateKey <= 20210131 - GROUP By SupplierKey, StockItemKey, IsOrderFinalized Which table distribution will minimize query times? A. replicated B. hash-distributed on PurchaseKey C. round-robin D. hash-distributed on lsOrderFinalized 20:59 / 3:02:47 Name Data type Nullable PurchaseKey Bigint No DateKey Int No SupplierKey Int No StockitemKey Int No PurchaseOrderID_ Int Yes OrderedQuantity | Int No OrderedOuters Int No ReceivedOuters _| Int No Package Nvarchar(50)__| No IsOrderFinalized _| Bit No LineageKey Int No<>No options available<>No explanation provided.|||Q20<>You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB. You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics. You need to prepare the files to ensure that the data copies quickly. Solution: You convert the files to compressed delimited text files. Does this meet the goal?<>No options available<>No explanation provided.|||Q21<>You are configuring data security settings for separate Azure SQL databases. Database A stores social security numbers, which you want to prevent any users or applications from viewing. The social security numbers appear in one column within a single table of Database A. Database B stores credit card information, including credit card numbers, which only privileged database administrators should be able to see. The credit card numbers appear in columns within several tables in Database B. How should you configure the data encryption settings for these databases to meet these requirements? A. B. c. Enable ‘Always Encrypted’ for Database A, and Dynamic Data Masking (DDM) for Database B. Enable ‘Always Encrypted’ for Database A and Database B. Enable Dynamic Data Masking (DDM) for Database A, and ‘Always Encrypted’ for Database B. Enable Dynamic Data Masking (DDM) for Database A and Database B.<>No options available<>No explanation provided.|||Q22<>You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB. You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics. You need to prepare the files to ensure that the data copies quickly. Solution: You modify the files to ensure that each row is more than 1 MB. Does this meet the goal? A. Yes B. No<>No options available<>No explanation provided.|||Q23<>You build a data warehouse in an Azure Synapse Analytics dedicated SQL pool. Analysts write a complex SELECT query that contains multiple JOIN and CASE statements to transform data for use in inventory reports. The inventory reports will use the data and additional WHERE parameters depending on the report. The reports will be produced once daily. You need to implement a solution to make the dataset available for the reports. The solution must minimize query times. What should you implement? A. an ordered clustered columnstore index B. a materialized view C. result set caching D. a replicated table<>No options available<>No explanation provided.|||Q24<>While configuring a data workflow, you are deciding how to load data into an Azure Synapse Analytics staging table. You would like the data to load into the staging table as quickly as possible. Which distribution type(s) would be optimal for this scenario? Round Robin Replicated Either Round Robin or Hash-Distributed Either Replicated or Hash-Distributed SOmPr<>No options available<>No explanation provided.|||Q25<>You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1. You plan to create a database named DB1 in Pool1. You need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool. Which format should you use for the tables in DB1? A. CSV B. ORC C. JSON D. Parquet<>No options available<>No explanation provided.|||Q26<>You are planning a solution to aggregate streaming data that originates in Apache Kafka and is output to Azure Data Lake Storage Gen2. The developers who will implement the stream processing solution use Java. Which service should you recommend using to process the streaming data? A. Azure Event Hubs B. Azure Data Factory C. Azure Stream Analytics D. Azure Databricks<>No options available<>No explanation provided.|||Q27<>You store files in an Azure Data Lake Storage Gen2 container. The container has the storage policy shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. ‘Answer Area “rules”: [ “enabled”: true, “contosorule”, “daysAfterCreationGreaterThan”: 60 ) “baseBlob”: ( “tierToCool”: { “daysAfterModi ficationGreaterThan”: 30 , “filters”: { “blobTypes” “blockBlob” ‘The files are [answer choice] after 30 days: v 1, “prefixMatch”: [ “container1/contoso”<>No options available<>No explanation provided.|||Q28<>You plan to use a U-SQL script within Azure Data Factory (ADF) to transform data in an Azure Storage blob container, then load the transformed data into an Azure Synapse Analytics table. You want to manage the entire data flow within a single schedule. Which choice below is the most reasonable option to correctly complete the task? A. B. Create one ADF pipeline for the U-SQL job to transform the data and then copy the output of the U-SQL job to Azure Synapse Analytics. Create two separate ADF pipelines. The first will use the U-SQL job to transform the data, and the second will copy the output of the U-SQL job to Azure Synapse Analytics. . Create one ADF pipeline for the U-SQL job to transform the data, and copy the output to Azure Synapse Analytics with Copy Data Wizard. . Create one ADF pipeline for the U-SQL job to transform the data with Copy Data Wizard, and then copy the output to Azure Synapse Analytics with Copy Data Wizard. 31:59 / 3:02:47<>No options available<>No explanation provided.|||Q29<>You are designing a financial transactions table in an Azure Synapse Analytics dedicated SQL pool. The table will have a clustered columnstore index and will include the following columns: c® TransactionType: 40 million rows per transaction type c> CustomerSegment: 4 million per customer segment © TransactionMonth: 65 million rows per month © AccountType: 500 million per account type. You have the following query requirements: c Analysts will most commonly analyze transactions for a given month. > Transactions analysis will typically summarize transactions by transaction type, customer segment, and/or account type You need to recommend a partition strategy for the table to minimize query times. On which column should you recommend partitioning the table? A. CustomerSegment B. AccountType C. TransactionType D. TransactionMonth<>No options available<>No explanation provided.|||Q30<>You have an Azure Data Lake Storage Gen2 account named account! that stores logs as shown in the following table. Type | Designated retention period You do not expect that the logs will be accessed during the retention Application [360 days periods. Infrastructure [60 days You need to recommend a solution for account! that meets the following requirements: c® Automatically deletes the logs at the end of each retention period ~ Minimizes storage costs What should you include in the recommendation? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area To minimize storage costs: Vv Store the infrastructure logs and the application logs in the Archive access tier Store the infrastructure logs and the application logs in the Cool access tier Store the infrastructure logs in the Cool access tier and the application logs in the Archive access tier To delete logs automatically: po > Pl @  33:59/3:0247<>No options available<>No explanation provided.|||Q31<>You plan to ingest streaming social media data by using Azure Stream Analytics. The data will be stored in files in Azure Data Lake Storage, and then consumed by using Azure Databricks and PolyBase in Azure Synapse Analytics. You need to recommend a Stream Analytics data output format to ensure that the queries from Databricks and PolyBase against the files encounter the fewest possible errors. The solution must ensure that the files can be queried quickly and that the data type information is retained. What should you recommend? A. JSON B. Parquet Cc. CSV D. Avro<>No options available<>No explanation provided.|||Q32<>You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool! contains a partitioned fact table named dbo.Sales and a staging table named stg.Sales that has the matching table and partition definitions. You need to overwrite the content of the first partition in dbo.Sales with the content of the same partition in stg.Sales. The solution must minimize load times. What should you do? A. Insert the data from stg.Sales into dbo.Sales. B. Switch the first partition from dbo.Sales to stg.Sales. C. Switch the first partition from stg.Sales to dbo.Sales. D. Update dbo.Sales from stg.Sales.<>No options available<>No explanation provided.|||Q33<>You are outlining the disaster recovery plan for your Azure SQL databases using active geo-replication. You plan to deploy your primary and secondary databases in different regions, to ensure greater availability in the event of a regional failure Both your primary and secondary databases’ firewalls should allow the same client IP address ranges, so that in the event the primary database fails the re-routed client-requests will successfully reach the secondary database. However, you do not want to allow more traffic than necessary to these or other databases in your production environment. What design choice below is recommended by Azure to ensure client requests are successfully received in the event of an unplanned failover? A. Make sure your database-level firewall's IP address rules are the same for your primary and secondary databases. B. Make sure your server-level firewall's IP address rules and database-level firewall's IP address rules are the same for your primary and secondary databases. C. Make sure your server-level firewall's IP address rules and VNet rules are the same for your primary and secondary databases. D. Make sure your database-level firewall's IP address rules and VNet rules are the same for your primary and secondary databases.<>No options available<>No explanation provided.|||Q34<>You have a Microsoft SQL Server database that uses a third normal form schema. You plan to migrate the data in the database to a star schema in an Azure Synapse Analytics dedicated SQL pool. You need to design the dimension tables. The solution must optimize read operations. What should you include in the solution? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Transform data for the dimension tables by: Maintaining to a third normal form Normalizing to a fourth normal form Denormalizing to a second normal form For the primary key columns in the dimension tables, use: New IDENTITY columns A new computed column The business key column from the source sys<>No options available<>No explanation provided.|||Q35<>You plan to develop a dataset named Purchases by using Azure Databricks. Purchases will contain the following columns: c® ProductID c® ItemPrice ce LineTotal c® Quantity > StorelD c® Minute c® Month ce Hour ce Year - c® Day You need to store the data to support hourly incremental load pipelines that will vary for each Store ID. The solution must minimize storage costs. How should you complete the code? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area df.write “bucket By oa) spartitionBy (*store1D”, *Hour”) sFange (*StoreID”, “Year”, “Month”, “Day”, “Hour”) 7 s0rtBy -mode (“append”) vesv(*/Purchases”) -json(*/Purchases”) -parquet (*/Purchases”) -saveAsTable (*/Purchases”)<>No options available<>No explanation provided.|||Q36<>You are designing a partition strategy for a fact table in an Azure Synapse Analytics dedicated SQL pool. The table has the following specifications: c® Contain sales data for 20,000 products. Use hash distribution on a column named ProductID. c® Contain 2.4 billion records for the years 2019 and 2020. Which number of partition ranges provides optimal compression and performance for the clustered columnstore index? A. 40 B. 240 Cc. 400 D. 2,400<>No options available<>No explanation provided.|||Q37<>You are managing a large relational database on SQL Server on Azure VM that is running SQL Server version 2017. This database needs to be joined with an external Hadoop data source, and then you must query both data sources using T-SQL. You have decided to implement Polybase to accomplish this task, but your query attempts using Polybase generate an error. What could have caused the implementation to fail? The SQL Server on Azure VM database exceeds the maximum possible row size of 32 KB. SQL Server 2017 does not support Polybase queries of Hadoop sources with T-SQL. Data should be migrated from the external Hadoop source to the database before using Polybase. D. Need to write custom query logic to join and integrate the data at the client level. PrP<>No options available<>No explanation provided.|||Q38<>You are creating dimensions for a data warehouse in an Azure Synapse Analytics dedicated SQL pool. You create a table by using the Transact-SQL statement shown in the following exhibit. CREATE TABLE [DBO] . [DimProduct] ( Use the drop-down menus to select the answer choice that eee a fieer aon ead NOT NULL, oduct Source ini 7 completes each statement based on the information presented in [ProductName] [nvarchar] (100) NOT NULL, the graphic. [ProductNumber] [nvarchar] (25) NOT NULL, NOTE: Each correct selection is worth one point. [Color] [nvarchar] (15) NULL, [Size] [nvarchar] (5) NULL, (Weight] [decimal] (8, 2) NULL, (ProductCategory] [nvarchar] (100) NULL, [SellStartDate] [date] NOT NULL, [SellEndDate] [date] NULL, [RowInsertedDateTime] [datetime] NOT NULL, [RowUpdatedDateTime] [datetime] NOT NULL, (ETLAuditID] [int] NOT NULL Answer Area DimProduct is @ fanswer choice] slowly changing dimension (SCD). v<>No options available<>No explanation provided.|||Q39<>You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns. FactPurchase will have 1 million rows of data added daily and will contain three years of data. Transact-SQL queries similar to the following query will be executed daily. SELECT - SupplierKey, StockltemKey, COUNT(*) FROM FactPurchase - WHERE DateKey >= 20210101 - AND DateKey <= 20210131 - GROUP By SupplierKey, StockltemKey Which table distribution will minimize query times? A. replicated B. hash-distributed on PurchaseKey C. round-robin D. hash-distributed on DateKey Pl) 42:59/3:02:47 Name Data type Nullable PurchaseKey Bigint No DateKey Int No SupplierKey Int No StockItemKey Int No PurchaseOrderlD Int Yes OrderedQuantity —_| Int No OrderedOuters Int No ReceivedOuters _| Int No Package Nvarchar(50) No IsOrderFinalized _| Bit No LineageKey Int No<>No options available<>No explanation provided.|||Q40<>You need to build a solution to ensure that users can query specific files in an Azure Data Lake Storage Gen2 account from an Azure Synapse Analytics serverless SQL pool. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select. Select and Place: Actions Answer Area<>No options available<>No explanation provided.|||Q41<>You are configuring an Azure SQL database with geo-replication enabled, and now want to configure Azure SQL Database auditing to monitor database events and ensure compliance. While the majority of databases on the Azure SQL database instance will focus on managing application data, one database will focus on processing environment logs as a way to maximize resource costs. The logs database's events will be very different from the other databases’ events. The primary issue is auditing event types or categories for a specific database that differ from the rest of the databases on the server. What is the most effective way to enable auditing for this Azure SQL Database? A. Enable server-level auditing on the primary database. B. Enable database-level auditing on the primary and the secondary database. C. Enable server-level auditing for the server and database-level auditing on logs database. D. Enable server-level auditing on the primary database and database-level auditing on the secondary database. > Pl 44:59/3:02:47<>No options available<>No explanation provided.|||Q42<>You are designing a data mart for the human resources (HR) department at your company. The data mart will contain employee information and employee transactions. From a source system, you have a flat extract that has the following fields: c® EmployeelD ce FirstName ce LastName c® Recipient c® GrossAmount c® TransactionID c® GovernmentID c> NetAmountPaid c® TransactionDate You need to design a star schema data model in an Azure Synapse Analytics dedicated SQL pool for the data mart. Which two tables should you create? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. a dimension table for Transaction B. a dimension table for EmployeeTransaction C. a dimension table for Employee D. a fact table for Employee<>No options available<>No explanation provided.|||Q43<>You are designing a dimension table for a data warehouse. The table will track the value of the dimension attributes over time and preserve the history of the data by adding new rows as the data changes. Which type of slowly changing dimension (SCD) should you use? A. Type 0 B. Type 1 C. Type 2 D. Type 3<>No options available<>No explanation provided.|||Q44<>You have data stored in thousands of CSV files in Azure Data Lake Storage Gen2. Each file has a header row followed by a properly formatted carriage return (/ r) and line feed (/n). You are implementing a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics by using PolyBase. You need to skip the header row when you import the files into the data warehouse. Before building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics. Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. NOTE: Each correct selection is worth one point Actions Answer Area Create a database scoped credential that uses Azure ‘Active Directory Application and a Service Principal Key location Create an extemal data source that uses the abfs 2 Use CREATE EXTERNAL TABLE AS SELECT (CETAS) and configure the reject options to<>No options available<>No explanation provided.|||Q45<>You are building an Azure Synapse Analytics dedicated SQL pool that will contain a fact table for transactions from the first half of the year 2020. You need to ensure that the table meets the following requirements: c® Minimizes the processing time to delete data that is older than 10 years c® Minimizes the I/O for queries that use year-to-date values How should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area CREATE TABLE (dbo). (FactTransaction) (TeansactiontypetD} int NOT MULL Datero) int NOT NULL D int Nor HULL (amount) money NOT NUE? ‘CLUSTERED COLUMNSTORE INDEX DrsrrBurion “TRUNCATE, TARGET.<>No options available<>No explanation provided.|||Q46<>You are designing a data table in Azure Table Storage for optimal performance when handling a large number of read requests and a much smaller number of write requests. As part of your design preparation, you have a list of the most common queries to expect for the table. Most queries will be point queries, with exact matches for the partition and row of the desired item. There will be row range scan queries, which include an exact match for the item partition and a partial match for the item row. Almost all queries include two key properties, a group ID, which corresponds to many items, and an item ID, which is specific to one item. With this information, which of the following design choices is optimal for your Azure Storage table? . Use the group ID as the partition key and item ID as the row key . Use the item ID as the partition key and the group ID as the row key . Concatenate the group ID and item ID into a composite partition key, and leave an empty string as the row key D. Use the group ID as the partition key, and leave an empty string as the row key Qnm>r > Pl 49:59/3:02:47<>No options available<>No explanation provided.|||Q47<>You are performing exploratory analysis of the bus fare data in an Azure Data Lake Storage Gen2 account by using an Azure Synapse Analytics serverless SQL pool. You execute the Transact-SQL query shown in the following exhibit. SELECT payment_type, SUM(fare_amount) AS fare_total FROM OPENROWSET ( BULK ‘csv/busfare/tripdata_2020*.csv’, DATA_SOURCE = ‘BusData’, FORMAT = ‘CSV’, PARSER_VERSION = ‘2.0’, FIRSTROW = 2 What do the query results include? A. Only CSV files in the tripdata_2020 subfolder. B. All files that have file names that beginning with “tripdata_2020". ) C. All CSV files that have file names that contain "tripdata_2020". wien ( D. Only CSV that have file names that beginning with payment_type INT 10, “tripdata_2020". fare_amount FLOAT 11 ) AS nyc GROUP BY payment_type ORDER BY payment_type;<>No options available<>No explanation provided.|||Q48<>You use PySpark in Azure Databricks to parse the following JSON input. “persons”: [ . a { You need to output the data in the following tabular format. “name” :"Keith”, age” :30, “dogs”: [“Fido”, “Fluffy” “name” :”Donna”, wage” :46, How should you complete the PySpark code? To answer, drag the sdogs”: ["spot”] appropriate values to the correct targets. Each value may be } used once, more than once, or not at all. You may need to drag ] the spit bar between panes or scroll to view content. J NOTE: Each correct selection is worth one point. Values Answer Area aaaE dbutils.fs.put (*/tmp/source.json”, source_json, True) source_df = spark.read.option(*multiline”, “true”) .json(*/tmp/source.json”) array_union persons = source df. Value Value ("persons") .alias(*persons”)) persons_dogs = persons.select (col (*persons.nane”) .alias ("owner"), col (*persons.age”) .alias (*age’ ‘explode [Vale rao") ‘createDataFrane<>No options available<>No explanation provided.|||Q49<>You are designing an application that will store petabytes of medical imaging data. When the data is first created, the data will be accessed frequently during the first week. After one month, the data must be accessible within 30 seconds, but files will be accessed infrequently. After one year, the data will be accessed infrequently but must be accessible within five minutes. You need to select a storage strategy for the data. The solution must minimize costs. Which storage tier should you use for each time frame? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area First week: 2 Archive Cool Hot After one month: Y Archive<>No options available<>No explanation provided.|||Q50<>A Data Engineer is designing a dimensional model data solution. Within this solution, slowly changing dimension (SCD) is used to effectively manage the change of dimension members. When there is a change detected in the source, the dimension table data should be overwritten with the latest value without any record of the previous value. Which SCD type will be most suitable here? Type 1 Type 2 Type 3 None of the listed types are suitable. SOm,r<>No options available<>No explanation provided.|||Q51<>You have an Azure Synapse Analytics Apache Spark pool named Pool1. You plan to load JSON files from an Azure Data Lake Storage Gen2 container into the tables in Pool1. The structure and data types vary by file. You need to load the files into the tables. The solution must maintain the source data types. What should you do? A. Use a Conditional Split transformation in an Azure Synapse data flow. B. Use a Get Metadata activity in Azure Data Factory. C. Load the data by using the OPENROWSET Transact-SQL command in an Azure Synapse Analytics serverless SQL pool. D. Load the data by using PySpark. You have an Azure Databricks workspace named workspace! in the Standard pricing tier. Workspace! contains an all- purpose cluster named cluster1. You need to reduce the time it takes for cluster1 to start and scale up. The solution must minimize costs. What should you do first? A. Configure a global init script for workspace1. B. Create a cluster policy in workspace’. C. Upgrade workspace! to the Premium pricing tier. D. Create a pool in workspace1.<>No options available<>No explanation provided.|||Q52<>You have the following Azure Stream Analytics query. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Answer Area Statements Yes The query combines two streams of partitioned data. [e) The stream scheme key and count must match the output scheme. roy Providing 60 streaming units will optimize the performance of the query. O- oo 0 WITH stepi AS (SELECT * FROM inputi PARTITION BY StateID INTO 10), step2 AS (SELECT * FROM input2 PARTITION BY StateID INTO 10) SELECT * INTO output FROM stepi PARTITION BY StateID UNION SELECT * INTO output FROM step2 PARTITION BY StateID<>No options available<>No explanation provided.|||Q53<>You are building a database in an Azure Synapse Analytics serverless SQL pool. You have data stored in Parquet files in an Azure Data Lake Storege Gen2 container. Records are structured as shown in the following sample. { "id": 123, “address_housenumber": "19c", “address_line": "Memory Lane", “applicant1_name" “applicant2_name" The records contain two applicants at most. You need to build a table that includes only the address fields. How should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area =) applications [CREATE EXTERNAL TABLE [CREATE TABLE [CREATE VIEW wrTH ( LOCATION = * ATA_SOURCE,<>No options available<>No explanation provided.|||Q54<>You have Azure Data Lake Storage which contains a very large amount of data. There are various pipelines triggered for analyzing the data that arrived that day, week, and month, and the ADLS store needs a data archival policy that meets the following requirements: +New data will be requested and updated thousands of times in the first 30 days. After 30 days, data will be accessed occasionally and should be available immediately. After 180 days data will be accessed very infrequently if at all. Which actions should be taken to meet these requirements in the most cost-effective way? (Choose 2 answers) A. B. Data will first be stored in the hot tier for the first 30 days, move to the cool tier after 30 days, and move to the archive tier after 180 days. Data will be stored in the cool tier for the first 30 days, move to the archive tier after 30 days, and deleted after 180 days. . Data will be stored in the cool tier for the first 180 days, and be deleted after 180 days. . Data will be stored in the hot tier for the first 30 days, move to the cool tier after 30 days, and stored offline after 180 days. i) 58:59 3:02:47<>No options available<>No explanation provided.|||Q55<>You have an Azure Synapse Analytics dedicated SQL pool named Pool! and an Azure Data Lake Storage Gen2 account named Account1. You plan to access the files in Account by using an external table. You need to create a data source in Pool1 that you can reference when you create the external table. How should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area CREATE EXTERNAL DATA SOURCE sourcel WITH ( LOCATION = ‘https://accountl. y¥ .core.windons.net’, Ba IPUSHDOWN = ON PE = BLOB_STORAGE ll<>No options available<>No explanation provided.|||Q56<>You have an Azure subscription that contains an Azure Blob Storage account named storage’ and an Azure Synapse Analytics dedicated SQL pool named Pool1. You need to store data in storage1. The data will be read by Pool1. The solution must meet the following requirements: Enable Pool to skip columns and rows that are unnecessary in a query. c® Automatically create column statistics. c> Minimize the size of files. Which type of file should you use? A. JSON B. Parquet C. Avro D. CSV<>No options available<>No explanation provided.|||Q57<>You plan to create a table in an Azure Synapse Analytics dedicated SQL pool. Data in the table will be retained for five years. Once a year, data that is older than five years will be deleted. You need to ensure that the data is distributed evenly across partitions. The solution must minimize the amount of time required to delete old data. How should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Select and Place: Values Answer Area Icostonerkey | «CREATE TABLE [dbo] . [FactSales} ‘CustomerKey ——————7 (Productkey] int NOT NULL HASH + (OrderDateKey} int NOT NULL ——————————_, _[customerkey] int NOT NULL ROUND_ROBIN , (SalesOrderNumber] nvarchar ( 20) NOT NULL (orderguantity) smallint NOT NULL a ; (unitPrice} money NOT NULL [ordexbaterey ]| “27! prdexdacekey, (| CLUSTERED COLUMNSTORE INDEX SalesOrderNumber , DISTRIBUTION = vas | ({ProductKey]}) @) 1:01<>No options available<>No explanation provided.|||Q58<>A company has to implement an application that would generate PDF files. The application would only need to store the PDF's and JSON metadata related to the PDF files. The PDF files would then be distributed over the web to various users. The PDF files could grow large in size. What Azure data and storage solutions are recommended for the application? A. Create Virtual machines and store the PDF's and JSON metadata on the virtual machines B. Create an Azure SQL database and store the PDF's and JSON metadata in a table on the database. C. Create an Azure Cosmos DB to store the metadata and the PDF's files. D. Use Blob storage to store the PDF's and the relevant JSON metadata in Azure Cosmos DB.<>No options available<>No explanation provided.|||Q59<>You have an Azure Data Lake Storage Gen2 service. You need to design a data archiving solution that meets the following requirements: c® Data that is older than five years is accessed infrequently but must be available within one second when requested. ce Data that is older than seven years is NOT accessed. -® Costs must be minimized while maintaining the required availability. How should you manage the data? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Data over five years old: Vv Delete the blob. Move to archive storage. Move to cool storage. Move to hot storage. Data over seven years old: Vv Delete the blob. 19 / 3:02:47<>No options available<>No explanation provided.|||Q60<>You plan to create an Azure Data Lake Storage Gen2 account. You need to recommend a storage solution that meets the following requirements: c® Provides the highest degree of data resiliency c® Ensures that content remains available for writes if a primary data center fails What should you include in the recommendation? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Replication mechanism: Change feed Zone-redundant storage (ZRS) Read-access geo-redundant storage (RA-GRS) Read-access geo-zone-redundant storage (RA-GRS) Failover process: Failover initiated by Microsoft itiated PI) 1:04:59 73:02:47<>No options available<>No explanation provided.|||Q61<>You need to implement a Type 3 slowly changing dimension (SCD) for product category data in an Azure Synapse Analytics dedicated SQL pool. You have a table that was created by using the following Transact-SQL statement. Which two columns should you add to the table? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A [EffectiveEndDate] [datetime] NULL, 8. [CurrentProductCategory] [nvarchar] (100) NOT NULL, c. [ProductCategory] [nvarchar] (100) NOT NULL, oD (EffectiveStartDate] [datetime] NOT NULL, E [OriginalProductCategory] [nvarchar] (100) NOT NULL, Correct Answer: BE CREATE TABLE [DBO] . [DimProduct] ( [ProductKey] [int] IDENTITY (1,1) NOT NULL, [ProductSourceID] [int] NOT NULL, [ProductNane] [nvarchar] (100) NOT NULL, [Color] [nvarchar] (15) NULL, [SellstartDate] [date] NOT NULL, [Sellend0ate] [date] NULL, [RowInsertedDateTime] [datetime] NOT NULL, [RowipdatedDateTine] [datetime] NOT NULL, (ETLAuditID] [int] NoT NULL )<>No options available<>No explanation provided.|||Q62<>You have created an Azure Synapse Analytics managed table backed by Parquet in Spark and query from a serverless SQL pool. The following command was used: CREATE TABLE mytestdbo.myparquettable(id int, name string, birthdate date) USING Parquet Then you add a row in the table with values as given. What will be the output of the following query which will be run after a few mins? SELECT id FROM mytestdbo.myparquettable WHERE birthdate = '01-01-2001' ERROR NULL A. B. Cc. D. ABC<>No options available<>No explanation provided.|||Q63<>You have an Azure subscription. You plan to build a data warehouse in an Azure Synapse Analytics dedicated SQL pool named pool! that will contain staging tables and a dimensional model. Pool! will contain the following tables. You need to design the table storage for pooll. The solution must meet the following requirements: c® Maximize the performance of data loading operations to Staging.WebSessions. c® Minimize query times for reporting queries against the dimensional model. Name ‘Number of rows Update frequency Description Common. Date 7,300 New rows * Contains one row per date Inserted yearly forthe last 20 years + Contains columns named Year, Month, Quarter, and |sWeekend Marketing WebSessions 4,500 500,000 Hourly inserts and updates | Fact table that contains counts of and updates sessions and page views, including foreign key values for date, channel, device, and medium Staging WebSessions '300,000 Hourly truncation and inserts Staging table for web session data, truncation and including descriptive fields for inserts channel, device, nd medium Which type of table distribution should you use for each table? To answer, drag the appropriate table distribution types to the correct tables. Each table distribution type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Table distribution types Answer Area Marketing Web Sessions: ‘Common.Data:<>No options available<>No explanation provided.|||Q64<>You have an Azure Synapse Analytics dedicated SQL pool. You need to create a table named FactinternetSales that will be a large fact table in a dimensional model. FactinternetSales will contain 100 million rows and two columns named SalesAmount and OrderQuantity. Queries executed on FactInternetSales will aggregate the values in SalesAmount and OrderQuantity from the last year for a specific product. The solution must minimize the data size and query execution time. How should you complete the code? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Correct Answer: C 59 / 3:02:47 CREATE TABLE [dbo}.[FactinternetSeles] ( [Productkey} int NOT MULL » (OrderDatexey] int NOT MULL » ([Customerkey] int NOT NULL » [Promotionkey] int NOT NULL » (SalesOrdertiumber} nvarchar(20) NOT MULL » [Orderquantity] smallint NOT MULL » [UnitPrice] money NOT NULL » [Salesémount] money NOT NULL<>No options available<>No explanation provided.|||Q65<>You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1. Table1 contains the following: ® One billion rows © A clustered columnstore index c® A hash-distributed column named Product Key c® A column named Sales Date that is of the date data type and cannot be null Thirty million rows will be added to Table1 each month. You need to partition Table1 based on the Sales Date column. The solution must optimize query performance and data loading. How often should you create a partition? A. once per month B. once per year C. once per day D. once per week<>No options available<>No explanation provided.|||Q66<>You have an Azure Databricks workspace that contains a Delta Lake dimension table named Table1. Table1 is a Type 2 slowly changing dimension (SCD) table. You need to apply updates from a source table to Table1. Which Apache Spark SQL operation should you use? A. CREATE B. UPDATE C. ALTER D. MERGE<>No options available<>No explanation provided.|||Q67<>The following JSON is an example of a Parquet datase ‘ on Azure Blob Storage. “referenceNane”: “<Azure Blob Storage linked service name>", Which of the JSON dataset properties is configured } “GFL S TEATS WASTED incorrectly? “Schema”: [ < physical schema, optional, retrievable during authoring > ],, “typeProperties”: zureBlobStorageLocation”, “: “containername”, folder/subfolder, A. "type": "Parquet" B. "schema": [< physical schema, optional, as retrievable during authoring > ], ) EGRESS TEED? "type": "AzureBlobStorageLocation", “compressionCodec": *LZ0" }<azure blob="" Linked="" name="" service= So storage=""> </azure> > rr<>No options available<>No explanation provided.|||Q68<>You are designing an Azure Data Lake Storage solution that will transform raw JSON files for use in an analytical workload. You need to recommend a format for the transformed files. The solution must meet the following requirements: © Contain information about the data types of each column in the files. c® Support querying a subset of columns in the files. c> Support read-heavy analytical workloads. > Minimize the file size. What should you recommend? B. CSV C. Apache Avro D. Apache Parquet<>No options available<>No explanation provided.|||Q69<>You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB. You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics. You need to prepare the files to ensure that the data copies quickly. Solution: You modify the files to ensure that each row is less than 1 MB. Does this meet the goal? A. Yes B. No<>No options available<>No explanation provided.|||Q70<>You plan to create a dimension table in Azure Synapse Analytics that will be less than 1 GB. You need to create the table to meet the following requirements: c® Provide the fastest query time. c® Minimize data movement during queries. Which type of table should you use? A. replicated B. hash distributed C. heap D. round-robin<>No options available<>No explanation provided.|||Q71<>You have to choose a real-time stream processing solution in Azure that meets the following requirements: 1.The solution must support Event Hubs, loT Hub, Kafka, and HDFS as input data sources. 2.The solution must support custom code as an input data format. 3.The solution needs built-in support for temporal processing. 4.Cosmos DB should be a supported sink. Which is the most suitable solution? A. B. Cc. D. . Azure Stream Analytics Apache Spark in Azure Databricks HDInsight with Storm . Azure Functions<>No options available<>No explanation provided.|||Q72<>You are designing a dimension table in an Azure Synapse Analytics dedicated SQL pool. You need to create a surrogate key for the table. The solution must provide the fastest query performance. What should you use for the surrogate key? A. a GUID column B. a sequence object C. an IDENTITY column<>No options available<>No explanation provided.|||Q73<>An electronics company needs to migrate Azure Data Lake Storage from ADLS Gen1 to Gen2 for a data analysis solution. The following are the key properties defining the current setup of ADLS Gen1" Data Organization with file and folder support *Ecosystem based on Azure Databricks 3.1 for big data analytics Encryption of data at rest using customer-managed key «Traffic accepted only from Specific VMs in an integrated Virtual Network VNet integration utilizes network service endpoint security for authentication to ADLS storage Which of the above properties requires refactoring or re-architecting to complete the migration successfully? (Choose 2 answers) Data Organization with file and folder support Ecosystem based on Azure Databricks 3.1 for big data analytics Traffic accepted only from Specific VMs in integrated Virtual Network VNet integration utilizes network service endpoint security for authentication to ADLS storage SORE Pe Plo 1:17:5973:02:47<>No options available<>No explanation provided.|||Q74<>You have an Azure Data Lake Storage Gen2 account that contains a container named container1. You have an Azure Synapse Analytics serverless SQL pool that contains a native external table named dbo.Table1. The source data for dbo.Table1 is stored in container1. The folder structure of container1 is shown in the following exhibit. The external data source is defined by a using the following statement. CREATE EXTERNAL DATA SOURCE DataLake same (LOCATION = ‘https: //mydatalake.dfs.core.windows.net/containert/foldert/**" » CREDENTIAL = DataLakeCred % For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Answer Area Statements Yes No When selecting all the rows in dbo.Table1, data from the mydata2.csv file will be returned. o 9°<>No options available<>No explanation provided.|||Q75<>You have an Azure Synapse Analytics dedicated SQL pool. You need to create a fact table named Table’ that will store sales data from the last three years. The solution must be optimized for the following query operations: + Show order counts by week. + Calculate sales totals by region. + Calculate sales totals by product. + Find all the orders from a given month. Which data should you use to partition Table1? A. product B. month C. week D. region<>No options available<>No explanation provided.|||Q76<>You are designing the folder structure for an Azure Data Lake Storage Gen2 account. You identify the following usage patterns: + Users will query data by using Azure Synapse Analytics serverless SQL pools and Azure Synapse Analytics serverless Apache Spark pools. + Most queries will include a filter on the current year or week. + Data will be secured by data source. You need to recommend a folder structure that meets the following requirements: + Supports the usage patterns + Simplifies folder security + Minimizes query times Which folder structure should you recommend? A. \DataSource\SubjectArea\YYYY\WW\FileData_YYYY_MM_DD.parquet B. \DataSource\SubjectArea\YYYY-WW\FileData_YYYY_MM_DD.parquet C. DataSource\SubjectArea\WW\YYYY\FileData_YYYY_MM_DD.parquet D. \YYYY\WW\DataSource\SubjectArea\FileData_YYYY_MM_DD.parquet E. WW\YYYY\SubjectArea\DataSource\FileData_YYYY_MM_DD.parquet @® HYUNDAI a 2024 ELANTRA. vf hyundaiusa.com cD oo es Got<>No options available<>No explanation provided.|||Q77<>The data team is designing a data analysis solution that includes several interdependent Azure Data Factory (ADF) pipelines. The first pipeline in a series will output values stored in Azure Synapse Analytics. Subsequent pipelines will need to reference the output value from the first pipeline and then proceed to an If Condition activity. What activity must the data team include in ADF pipelines to reference the external value stored in ADF and then proceed to the If Condition activity based on that value? Wait Activity ForEach Activity Set Variable Lookup Activity SORFr<>No options available<>No explanation provided.|||Q78<>You have an Azure subscription that contains an Azure Synapse — Analytics dedicated SQL pool. (CREATE HOLE (dbo) .{Sales) You plan to deploy a solution that will analyze sales data and include ‘ ensue) sine sor wee the following: Icustomeetd] int NOT WILL country) Ant WoT WL + Atable named Country that will contain 195 rows + (Total) money MOF MLL + A table named Sales that will contain 100 million rows : + A query to identify total sales by country and customer from the = past 30 days rsrareution = ad HASH@{Customertd) ; _ HasH(OrderDate) You need to create the tables. The solution must maximize query REPLICATE performance. — cuustene coumnsrone 00% How should you complete the script? To answer, select the > appropriate options in the answer area. os pee SL ee) countrytd) int nor wut NOTE: Each correct selection is worth one point. [coumteyCode) varchar(20) WoT WALL > want «<>No options available<>No explanation provided.|||Q79<>You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named account! and an Azure Synapse Analytics workspace named workspace1. You need to create an external table in a serverless SQL pool in workspace’. The external table will reference CSV files stored in account1. The solution must maximize performance. How should you configure the external table? A. Use a native external table and authenticate by using a shared access signature (SAS). B. Use a native external table and authenticate by using a storage account key. C. Use an Apache Hadoop external table and authenticate by using a shared access signature (SAS). D. Use an Apache Hadoop external table and authenticate by using a service principal in Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra. ‘DimGeography Date<>No options available<>No explanation provided.|||Q80<>You have an Azure Synapse Analytics serverless SQL epee pees pool that contains a database named db1. The data model ‘StateProvince 1 Month . . . we, Region Year for db1 is shown in the following exhibit. ont Calarter Use the drop-down menus to select the answer choice that completes each statement based on the information one presented in the exhibit. CustomeriD (Ga CustomerName [] eae a . CustomerType FactOrders NOTE: Each correct selection is worth one point. Geographykey Customerkey an StoreKey Dimstore PrecuctKary StoreKey OrderDateKey ent StorelD | OrderNumber ProductiD StoreName OrderLineNumber | -—) —productName sucreh Cd ProductLinelD Geograp! cI To convert the data model to a star schema, [answer choice). o<>No options available<>No explanation provided.|||Q81<>The following shows a table in a dedicated SQL CED Pool in Azure Synapse Analytics: A user was given permission to access the table using the following statement: menber1 menber2 GRANT SELECT ON Membership(Member!D, Name, Phone, Amount) TO TestUser; After entering the data into tables, the newly created TestUser tuns the following query SELECT * FROM Membership; What is the expected output of this query? A. NULL B. ERROR Cc. D. The entire table . The entire table except the SSN column menber3<>No options available<>No explanation provided.|||Q82<>You have an Azure Databricks workspace and an Azure Data Lake Storage Gen2 account named storage1. New files are uploaded daily to storage1. You need to recommend a solution that configures storage’ as a structured streaming source. The solution must meet the following requirements: + Incrementally process new files as they are uploaded to storage. + Minimize implementation and maintenance effort. + Minimize the cost of processing millions of files. + Support schema inference and schema drift. Which should you include in the recommendation? A. COPY INTO B. Azure Data Factory C. Auto Loader D. Apache Spark FileStreamSource<>No options available<>No explanation provided.|||Q83<>You have an Azure subscription that contains the resources shown in the following table. storage! | Azure Blob storage Contains publicly accessible TSV files that account do NOT have a header row [st [serene nneees Contains a serverless SQL pool You need to read the TSV files by using ad-hoc queries and the OPENROWSET function. The solution must assign a name and override the inferred data type of each column. What should you include in the OPENROWSET function? A. the WITH clause B. the ROWSET_OPTIONS bulk option C. the DATAFILETYPE bulk option D. the DATA_SOURCE parameter<>No options available<>No explanation provided.|||Q84<>You have an Azure Synapse Analytics dedicated SQL pool that contains a table named DimSalesPerson. DimSalesPerson contains the following columns: + RepSourcelD + SalesRepID + FirstName + LastName * StartDate + EndDate + Region You are developing an Azure Synapse Analytics pipeline that includes a mapping data flow named Dataflow1. Dataflow1 will read sales team data from an external source and use a Type 2 slowly changing dimension (SCD) when loading the data into DimSalesPerson. You need to update the last name of a salesperson in DimSalesPerson. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Update three columns of an existing row. B. Update two columns of an existing row. C. Insert an extra row. D. Update one column of an existing row.<>No options available<>No explanation provided.|||Q85<>A data team is designing several data processing solutions with complex Azure Data Factory (ADF) pipelines. These pipelines are dependent upon the successful completion of HDInsight processing jobs that will be initiated simultaneously with the ADF pipelines. In order for the ADF pipeline to complete successfully, the HDInsight job output must be available. Which ADF control flow activity would work best when the pipeline must confirm and evaluate the HDInsight output before proceeding to the next task? A. Await activity B. Awebhook activity Cc. D. . A validation activity |. Anif condition activity<>No options available<>No explanation provided.|||Q86<>You plan to use an Azure Data Lake Storage Gen2 account to implement a Data Lake development environment that meets the following requirements: + Read and write access to data must be maintained if an availability zone becomes unavailable. + Data that was last modified more than two years ago must be deleted automatically. + Costs must be minimized. What should you configure? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. ‘Answer Area For storage redundancy: 2 Geo-zone-redundant storage (GZRS) Locally-redundant storage (LRS) Zone-redundant storage (ZS) For data deletion: aa A lifecycle management policy Soft delete | Versioning<>No options available<>No explanation provided.|||Q87<>You are developing an Azure Synapse Analytics pipeline that will include a mapping data flow named Dataflow1. Dataflow1 will read customer data from an external source and use a Type 1 slowly changing dimension (SCD) when loading the data into a table named DimCustomer in an Azure Synapse Analytics dedicated SQL pool. You need to ensure that Dataflow1 can perform the following tasks: + Detect whether the data of a given customer has changed in the DimCustomer table. + Perform an upsert to the DimCustomer table. Which type of transformation should you use for each task? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. ‘Answer Area Detect whether the data of a given customer has changed in the DimCustomer table: ms Aggregate | Derived column Surrogate key Perform an upsert to the DimCustomer table: o Alter row Assert Cast<>No options available<>No explanation provided.|||Q88<>A data team is designing a data processing solution with an Azure Data Factory (ADF) pipeline that must call a custom REST endpoint from a Data Factory pipeline. Which ADF control activity will do this? . Wait Activity ForEach Activity . Web Activity . Lookup Activity pom><>No options available<>No explanation provided.|||Q89<>You have an Azure Synapse Analytics serverless SQL pool. You have an Azure Data Lake Storage account named adls1 that contains a public container named container1. The container1 container contains a folder named folder1. You need to query the top 100 rows of all the CSV files in folder1. How should you complete the query? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point ‘Values Answer Area BULK SELECT TOP 100 * DATA_SOURCE . somal ¢ [lpcaTion . |‘https://adlsi.dfs.core.windows .net/containerl/folderl/*.csv', © romat = *€5v") AS rows<>No options available<>No explanation provided.|||Q90<>You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1. You plan to create a database named DB1 in Pool1. You need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool. Which format should you use for the tables in DB1? A. Parquet B. ORC C. JSON D. HIVE<>No options available<>No explanation provided.|||Q91<>You have an Azure Data Lake Storage Gen2 account named storage1. You plan to implement query acceleration for storage1. Which two file types support query acceleration? Each correct answer presents a complete solution. NOTE: Each correct selection is worth one point. A. JSON B. Apache Parquet Cc. XML D. CSV E. Avro<>No options available<>No explanation provided.|||Q92<>As a data engineer managing a company's Azure workloads, you need to upload 22 TB of data in Azure Storage into an Azure Synapse dedicated SQL pool. The 22 TB of data is Hive data in Optimized Row Columnar (ORC) format. After starting the upload, Azure displayed Java out-of-memory errors. You are not required to upload all of the data at once. Which steps could you take to complete the upload without generating similar errors? Use compressed delimited text files Export only a subset of the columns Colocate your storage layer and your dedicated SQL pool None of these options will prevent an error. com><>No options available<>No explanation provided.|||Q93<>You have an Azure subscription that contains the resources shown in the following table. Name Type Description ‘StorageT ‘Azure Blob storage Contains publicly accessible JSON files account WSt ‘Azure Synapse Analyics | Contains a serverless SL pool workspace You need to read the files in storage1 by using ad-hoc queries and the OPENROWSET function. The solution must ensure that each rowset contains a single JSON record. To what should you set the FORMAT option of the OPENROWSET function? A. JSON B. DELTA C. PARQUET D. CSV<>No options available<>No explanation provided.|||Q94<>You have an Azure subscription that contains the Azure Synapse Analytics workspaces shown in the following table. Each workspace must read and write data to datalake1. Each workspace contains an unused Apache Spark pool. You plan to configure each Spark pool to share catalog objects that reference datalake1. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point. Answer Area Statements Yes The shared catalog objects can be stored in Azure Database for MySQL. © For the Apache Hive Metastore of each workspace, you must configure © 4 linked service that uses user-password authentication. ‘The users of workspace1 must be assigned the Storage Blob Contributor © role for datalake1. Name Primary storage account workspaceT datalaKet workspace? datalake2 workspaces datalaket ° °<>No options available<>No explanation provided.|||Q95<>An application running on a third-party device has been programmed to send a stream of events to an loT Hub within Azure. You have set up an Azure Stream Analytics streaming job to use this loT Hub as a source. Your analysis team would like the data to be made available so that a Power BI dashboard will be updated automatically. Which sink type should be used? . Azure Data Factory . Azure Cosmos DB - Power BI Dataflow . Power BI Dataset poOmEr<>No options available<>No explanation provided.|||Q96<>nan<>No options available<>No explanation provided.|||Q97<>You have a data warehouse. You need to implement a slowly changing dimension (SCD) named Product that will include three columns named ProductName, ProductColor, and ProductSize. The solution must meet the following requirements: + Prevent changes to the values stored in ProductName. + Retain only the current and the last values in ProductSize. + Retain all the current and previous values in ProductColor. Which type of SCD should you implement for each column? To answer, drag the appropriate types to the correct columns. Each type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. SCD Type Answer Area Type 0 Type 1 Type 2 Type 3 ProductName: Size:<>No options available<>No explanation provided.|||Q98<>You have an Azure subscription that contains an Azure Synapse Analytics workspace named ws1 and an Azure Cosmos DB database account named Cosmos1. Cosmos contains a container named container1 and ws! contains a serverless SQL pool. You need to ensure that you can query the data in container1 by using the serverless SQL pool. Which three actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Enable Azure Synapse Link for Cosmos1. B. Disable the analytical store for container1. C. In ws, create a linked service that references Cosmos]. D. Enable the analytical store for container. E. Disable indexing for container. ~)<>No options available<>No explanation provided.|||Q99<>One of the largest fintech companies wants to utilize Azure Synapse Analytics for their new product, and expect that customers’ Personal Identifiable Information (Pll) will be stored in the databases. In addition, company employees on the data team members will require access to the dedicated SQL Pool created in Azure Synapse Analytics. How can you make sure that the Pll involved cannot be viewed by the data team? . Use Transparent Data Encryption (TDE) Use Dynamic Data Masking (DDM) Create user accounts without administrator privileges for data team members . Assign the SQL Security Manager role to data team members comPr<>No options available<>No explanation provided.|||Q100<>You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named account! and a user named User. In account1, you create a container named container’. In container1, you create a folder named folder. You need to ensure that User1 can list and read all the files in folder. The solution must use the principle of least privilege. How should you configure the permissions for each folder? To answer, drag the appropriate permissions to the correct folders. Each permission may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Permissions Answer Area Execute None container1/: Read Read and Execute container 1/folder1;<>No options available<>No explanation provided.|||Q101<>You have an Azure Data Factory pipeline named pipeline1. You need to execute pipeline1 at 2 AM every day. The solution must ensure that if the trigger for pipeline stops, the next pipeline execution will occur at 2 AM, following a restart of the trigger. Which type of trigger should you create? A. schedule B. tumbling C. storage event D. custom event<>No options available<>No explanation provided.|||Q102<>You have an Azure data factory named adf1 that contains a pipeline named ExecProduct. ExecProduct contains a data flow named Product. The Product data flow contains the following transformations: 1. WeeklyData: A source that points to a CSV file in an Azure Data Lake Storage Gen2 account with 20 columns 2. ProductColumns: A select transformation that selects from WeeklyData six columns named ProductID, ProductDescr, ProductSubCategory, ProductCategory, ProductStatus, and ProductLastUpdated 3. ProductRows: An aggregate transformation 4. ProductList: A sink that outputs data to an Azure Synapse Analytics dedicated SQL pool The Aggregate settings for ProductRows are ‘Aggregate settings Optimize Inspect Data preview eee: — ae — —o Grouped by: Producti Add a} a Gi Open expression builder cot topeion configured as shown in the following exhibit. answer Area Statements There will be six columns in the output of ProductRows. Leaen more C3<>No options available<>No explanation provided.|||Q103<>You are an Azure SQL Database Software as a Service developer, and suddenly your app undergoes tremendous demand. You need to accommodate the growth so you add more databases (shards). How do you redistribute the data to the new databases without disrupting the data integrity? A. Use the split-merge tool to move data from constrained databases to the new databases. B. Perform differential backups of the original data and incrementally restore the data to the new database shards. C. Use Azure Active Directory authentication as a mechanism of creating and connecting to the new Azure SQL Databases. D. Use Azure data dependent routing to gather data in a query request and route the data to the new Azure SQL Databases.<>No options available<>No explanation provided.|||Q104<>You manage an enterprise data warehouse in Azure Synapse Analytics. Users report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries. You need to monitor resource utilization to determine the source of the performance issues. Which metric should you monitor? A. DWU limit B. Cache hit percentage C. Local tempdb percentage D. Data IO percentage<>No options available<>No explanation provided.|||Q105<>You have an Azure Synapse Analytics serverless SQL pool. You have an Apache Parquet file that contains 10 columns. You need to query data from the file. The solution must return only two columns. How should you complete the query? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area SELECT * FROM ‘OPENRONSET( Y _ N'https://myaccount .dfs.core.windows .net/mycontainer/mysubfolder/data.parquet’, FORMAT = "PARQUET"<>No options available<>No explanation provided.|||Q106<>You plan to create a real-time monitoring app that alerts users when a device travels more than 200 meters away from a designated location. You need to design an Azure Stream Analytics job to process the data for the planned app. The solution must minimize the amount of code developed and the number of technologies used. What should you include in the Stream Analytics job? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Input type: | vw Stream Reference<>No options available<>No explanation provided.|||Q107<>An electronics company utilizes Azure Data Lake Storage (ADLS) Generation 1 for Big Data Analytics. As part of the data analytics team, your new assignment is to plan and design the migration of ADLS Generation 1 to ADLS Generation 2. Only a small number of existing pipelines are connected to the current data lakes, but your team requires that the migration results in no downtime for any related applications and that the process requires minimal administration. Which of the following migration methods would best meet these requirements? Lift and Shift Incremental Copy Dual Pipeline Bidirectional Sync PHP<>No options available<>No explanation provided.|||Q108<>A company has a real-time data analysis solution that is hosted on Microsoft Azure. The solution uses Azure Event Hub to ingest data and an Azure Stream Analytics cloud job to analyze the data. The cloud job is configured to use 120 Streaming Units (SU). You need to optimize performance for the Azure Stream Analytics job. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. Implement event ordering. B. Implement Azure Stream Analytics user-defined functions (UDF). C. Implement query parallelization by partitioning the data output. D. Scale the SU count for the job up. E. Scale the SU count for the job down. F. Implement query parallelization by partitioning the data input.<>No options available<>No explanation provided.|||Q109<>You need to trigger an Azure Data Factory pipeline when a file arrives in an Azure Data Lake Storage Gen2 container. Which resource provider should you enable? A. Microsoft.Sql B. Microsoft.Automation C. Microsoft.EventGrid D. Microsoft.EventHub<>No options available<>No explanation provided.|||Q110<>You plan to perform batch processing in Azure Databricks once daily. Which type of Databricks cluster should you use? A. High Concurrency B. automated C. interactive<>No options available<>No explanation provided.|||Q111<>You are processing streaming data from vehicles that pass through a toll booth. You need to use Azure Stream Analytics to return the license plate, vehicle make, and hour the last vehicle passed during each 10- minute window. How should you complete the query? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area WITH LastInWindow AS ( ) SELECT SELECT ‘COUNT MAX MIN TOPONE FROM Input TIMESTAMP BY Time GROUP BY [¥] (minute, HoppingWindow SessionWindow SlidingWindow TumblingWindow Input .License_plate, Input Make, Input .Time Input TIMESTAMP BY Time INNER JOIN LastInWindow lv] (Time) AS LastEventTime<>No options available<>No explanation provided.|||Q112<>nan<>No options available<>No explanation provided.|||Q113<>You have an Azure Data Factory instance that contains two pipelines named Pipeline? and Pipeline2. Pipeline’ has the activities shown in the following exhibit. Stored procedure ‘Set variable = stored procedure -—» (2X) Set variablet . Pipeline2 has the activities shown in the following exhibit. Execute Pipeline 7 Set variable » Execute Pipelinet ™ * (.X) Set variablet You execute Pipeline2, and Stored procedure! in Pipeline’ fails. What is the status of the pipeline runs? A. Pipeline1 and Pipeline2 succeeded. B. Pipeline and Pipeline? failed. C. Pipeline1 succeeded and Pipeline2 failed. D. Pipeline’ failed and Pipeline2 succeeded first_name | last_name | hire_date | employee_type Jane Doe 2019-08-23 | new<>No options available<>No explanation provided.|||Q114<>You have the following table named Employees. Ben ‘Smith 2017-12-15 | Standard You need to calculate the employee_type value based on the hire_date value. How should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Select and Place: Values Answer Area SELECT CASE : a WHEN hire date >= ‘2019-01-01' THEN ‘New’ END AS employee_type<>No options available<>No explanation provided.|||Q115<>You want to modify user-defined functions using T-SQL. The functions do not require a specific user-defined type, and you have ALTER permission on the schema. However, your ALTER FUNCTION requests fail. What will be the best possible cause? You are trying to change a scalar-valued function to a table-valued function. You must have ALTER permission on both the schema and on the function. You need EXECUTE permission on the user-defined type. The request altered related dependent functions and stored procedures. 9:op> You can modify user-defined functions in SQL Server by using SQL Server Management Studio or Transact-SQL. Modifying user- defined functions as described below will not change the functions’ permissions, nor will it affect any dependent functions, stored procedures, or triggers. Changing a user-defined function requires ALTER permission on the function or on the schema, not both. If the function specifies a user-defined type, you need EXECUTE permission on the type, but in this scenario EXECUTE permission is not required. When you use the ALTER function there are certain limitations ALTER FUNCTION cannot be used to perform any of the following actions: + Change a scalar valued function to a table-valued function, or vice versa. * Change a e function to a multistatement function, or vice versa.<>No options available<>No explanation provided.|||Q116<>You have an Azure Synapse Analytics workspace named WS1. You have an Azure Data Lake Storage Gen2 container that contains JSON-formatted files in nei ‘2020-06-10"19:49:34.5592", the following format. mente You need to use the serverless SQL pool in WS1 to read the files. How should you complete the Transact-SQL statement? To answer, drag the appropriate , values to the correct targets. Each value may be used once, more than once, or not at all. Satan t You may need to drag the split bar between panes or scroll to view content. Teurtemerzatort NOTE: Each correct selection is worth one point. 2 Values Answer Area » : select Row ‘ BULK ‘thttps: //contoso.blob.core. windows net/contosodw’ » 1 fopendatasource] tieldterminato a Helaquore = cpendeon Forverminacor , Speeqaeryin] vith, (id varchax (50), contosaateventfine varchar(S0) °$.context.data.eventTine’, Contextdatensmplingrate varchar (50) se-contex® an 7 contextdataissynthetic varchar(50) ‘$.context.data..<>No options available<>No explanation provided.|||Q117<>You have an Apache Spark DataFrame named temperatures. A sample of the data is shown in the following table. You need to produce the following table by using a Spark SQL query. Year JAN FEB MAR: APR MAY 2019 23 41 52 76 92 2020 24 42 [49 78 91 2021 26 53 [34 79 95 How should you complete the query? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Values pa Answer Area SELECT * FROM ( SELECT YEAR(Date) Year, MONTH(Date) Month, Temp FROM temperatures WHERE date BETWEEN DATE ‘2019-01-01’ AND DATE ‘2021-08-31 ) G AVG ([~ (Temp AS DECIMAL(4, 1))) FOR Month in ( PI) 2:03:59 73:02:47<>No options available<>No explanation provided.|||Q118<>You have an Azure Data Factory that contains 10 pipelines. You need to label each pipeline with its main purpose of either ingest, transform, or load. The labels must be available for grouping and filtering when using the monitoring experience in Data Factory. What should you add to each pipeline? A. a resource tag B. a correlation ID C. arun group ID D. an annotation<>No options available<>No explanation provided.|||Q119<>A data engineer has two tables of structured data that needs to be stored in an Azure Synapse Analytics. The Sales table consists of 3 billion rows and 4 columns. The three main columns used for analysis are OrderDate, CustomerReference, and ProductID. Approximately 10 million rows will be added each day. The Products table will hold product information such as color, size, category, subcategory, list price, manufacturing cost, and supplier. Most of the analytics would require joins between the two tables. How should the distribution of the tables be set up within the SQL Synapse Analytics? A. Sales - hash (OrderDate), Products - round_robin B. Sales - replicated, Products - round_robin C. Sales - hash (Productid), Products - replicated D. Sales - hash (Productid), Products - hash (Productid)<>No options available<>No explanation provided.|||Q120<>The following code segment is used to create an Azure Databricks cluster. For each of the following statements, select Yes if the statement is true. Otherwise, select No. NOTE: Each correct selection is worth one point.<>No options available<>No explanation provided.|||Q121<>You are designing a statistical analysis solution that will use custom proprietary Python functions on near real-time data from Azure Event Hubs. You need to recommend which Azure service to use to perform the statistical analysis. The solution must minimize latency. What should you recommend? A. Azure Synapse Analytics B. Azure Databricks C. Azure Stream Analytics D. Azure SQL Database<>No options available<>No explanation provided.|||Q122<>You have an enterprise data warehouse in Azure Synapse Analytics that contains a table named FactOnlineSales. The table contains data from the start of 2009 to the end of 2012. You need to improve the performance of queries against FactOnlineSales by using table partitions. The solution must meet the following requirements: ~ Create four partitions based on the order date. c» Ensure that each partition contains all the orders placed during a given calendar year. How should you complete the T-SQL command? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area CREATE TABLE [dbo] .FactOnlineSales ({OnlineSalesKey] [int] NOT NULL, (OrderDateKey] [datetime] NOT NULL, {StoreKey] [int] NOT NULL, (ProductKey] [int] NOT NULL, (CustomerKey] [int] NOT NULL, {SalesOrderNumber] [nvarchar] (20) NOT NULL, {SalesQuantity] [int] NOT NULL, {SalesAmount] [money] NOT NULL, (unitPrice] [money] NULL) WITH (CLUSTERED COLUMNSTORE INDEX) PARTITION ([OrderDateKey] RANGE 08:59 / 3:02:47<>No options available<>No explanation provided.|||Q123<>A Data Engineer is setting up an Azure Data Factory pipeline to move data from an on-premises Microsoft SQL Server Database to Azure Synapse Analytics. The source table will be loaded by an internal process between 1:00 am and 3:00 am. The loading of the data should be started at 3:30 am. The transfer should be performed in the most efficient manner. What objects types will be required to setup this Azure Data Factory? A. Azure Data Lake Storage, Linked Service (x2), Polybase, Dataset (x2), Pipeline, Trigger B. Linked Service, Dataset, Pipeline, Trigger Cc. D. Linked Service (x2), Polybase, Dataset (x2), Pipeline (x2) . Azure Data Lake Storage, Linked Service (x2), Polybase, Dataset (x2), Pipeline<>No options available<>No explanation provided.|||Q124<>You need to implement a Type 3 slowly changing dimension (SCD) for product category data in an Azure Synapse Analytics dedicated SQL pool. You have a table that was created by using the following Transact-SQL statement. Which two columns should you add to the table? Each correct CME Teepe teeol (besbeogvce) -t [ProductKey} [int] IDENTITY (1,1) NOT NULL, answer presents part of the solution. [ProductSourceID] [int] NOT NULL, NOTE: Each correct selection is worth one point. (ProductName) [nvarchar] (100) NULL, [color] [varchar] (15) NULL, [sellstartDate] [date] NOT NULL, [sellgndDate] [date] NULL, [RowInsertedDateTime] [datetime] NOT NULL, [RowUpdatedDateTime] [datetime] NOT NULL, [ETEAuditID] [int] NOT NULL ) A. [EffectiveStartDate] [datetime] NOT NULL, B. [CurrentProductCategory] [nvarchar] (100) NOT NULL, C. [EffectiveEndDate] [datetime] NULL, D. [ProductCategory] [nvarchar] (100) NOT NULL, E. [OriginalProductCategory] [nvarchar] (100) NOT NULL,<>No options available<>No explanation provided.|||Q125<>You are designing an Azure Stream Analytics solution that will analyze Twitter data. You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once. Solution: You use a hopping window that uses a hop size of 5 seconds and a window size 10 seconds. Does this meet the goal?<>No options available<>No explanation provided.|||Q126<>You are building an Azure Stream Analytics job to identify how much time a user spends interacting with a feature on a webpage. The job receives events based on user actions on the webpage. Each row of data represents an event. Each event has a type of either ‘start’ or ‘end’. You need to calculate the duration between start and end events. How should you complete the query? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. ‘Answer Area SELECT [user], feature, ina DATEADD ( DATEDIFF( DATEPART ( ‘econd, J] (rine) oven (waRTITION BY {user}, feature LIMIT DURATION(hoUr, 1) WHEN Event = ‘start*), Time) as duration FROM input TIMESTAMP BY Time WHERE ;<>No options available<>No explanation provided.|||Q127<>You are the data engineer for a very large e-commerce website with a global userbase. You have a data pipeline that gathers clickstream with Azure Event Hub and sends it to Azure Stream Analytics. You need to design a query that will: Aggregate the number of clicks into distinct periods of time «Divide the numbers based on the user region Count each click once and only once Which of the following functions should be used in your query? A. a tumbling window function B. asession window function C. a sliding window function D. a hopping window function<>No options available<>No explanation provided.|||Q128<>You are creating an Azure Data Factory data flow that will ingest data from a CSV file, cast columns to specified types of data, and insert the data into a table in an Azure Synapse Analytic dedicated SQL pool. The CSV file contains three columns named username, comment, and date. The data flow already contains the following: ce A source transformation. c® A Derived Column transformation to set the appropriate types of data. c® A sink transformation to land the data in the pool. You need to ensure that the data flow meets the following requirements: c® All valid rows must be written to the destination table. c® Truncation errors in the comment column must be avoided proactively. c® Any rows containing comment values that will cause truncation errors upon insert must be written to a file in blob storage. Which two actions should you perform? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point. A. To the data flow, add a sink transformation to write the rows to a file in blob storage. B. To the data flow, add a Conditional Split transformation to separate the rows that will cause truncation errors. C. To the data flow, add a filter transformation to filter out rows that will cause truncation errors. D. Add a select transformation to select only the rows that will cause truncation errors.<>No options available<>No explanation provided.|||Q129<>You need to create an Azure Data Factory pipeline to process data for the following three departments at your company: Ecommerce, retail, and wholesale. The solution must ensure that data can also be processed for the entire company. How should you complete the Data Factory data flow script? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Values Answer Area all, ecommerce, retail, wholesale | CleanData split ( ‘ecommerce’, dept==*retail’, ‘wholesale’ ‘ecommerce’, dept== ‘wholesale’, dept=='retail’ disjoint: false ) ~> SplitByDepte ( ) disjoint: true ecommerce, retail, wholesale, all<>No options available<>No explanation provided.|||Q130<>You have an Azure Data Lake Storage Gen2 account that contains a JSON file for customers. The file contains two attributes named FirstName and LastName. You need to copy the data from the JSON file to an Azure Synapse Analytics table by using Azure Databricks. A new column must be created that concatenates the FirstName and LastName values. You create the following components: c® A destination table in Azure Synapse c An Azure Blob storage container c® A service principal Which five actions should you perform in sequence next in is Databricks notebook? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order. Actions Answer Area Mount the Data Lake Storage onto DBFS. Write the results to a table in Azure Synapse. Perform transformations on the file. Specify a temporary folder to stage the data. Write the results to Data Lake Storage. Read the file into a data frame.<>No options available<>No explanation provided.|||Q131<>Which of the following statements regarding Azure Stream Analytics’ event ordering feature to manage late- arrived data is incorrect? A. Early events are sent to the output if the sender's clock is running too fast. B. The System.Timestamp value is altered during event ingestion for very old events. C. If input events are infrequent and sparse, the output can be delayed by that amount of time. D. The event ordering feature allows Azure Stream Analytics to process data from any historical date.<>No options available<>No explanation provided.|||Q132<>You build an Azure Data Factory pipeline to move data from an Azure Data Lake Storage Gen2 container to a database in an Azure Synapse Analytics dedicated SQL pool. Data in the container is stored in the following folder structure. Jin /{YYYY}/{MM}/{DD}/{HH}/{mm} The earliest folder is /in/2021/01/01/00/00. The latest folder is /in/2021/01/15/01/45. You need to configure a pipeline trigger to meet the following requirements: c® Existing data must be loaded. c® Data must be loaded every 30 minutes. c® Late-arriving data of up to two minutes must be included in the load for the time at which the data should have arrived. How should you configure the pipeline trigger? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Type: Vv Event On-demand Schedule Tumbling window Additional properties:<>No options available<>No explanation provided.|||Q133<>You are designing a near real-time dashboard solution that will visualize streaming data from remote sensors that connect to the internet. The streaming data must be aggregated to show the average value of each 10-second interval. The data will be discarded after being displayed in the dashboard. The solution will use Azure Stream Analytics and must meet the following requirements: c® Minimize latency from an Azure Event hub to the dashboard. c® Minimize the required storage. c® Minimize development effort. Answer Area What should you include in the solution? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point Azure Stream Analytics input type: lv Azure Event Hub Azure SQL Database Azure Stream Analytics Microsoft Power BI Azure Stream Analytics output type: iv Azure Event Hub Azure SQL Database Azure Stream Analytics Microsoft Power BI Aggregation query location:<>No options available<>No explanation provided.|||Q134<>You are configuring an Azure SQL database with geo-replication enabled, and now want to configure Azure SQL Database auditing to monitor database events and ensure compliance. The critical consideration is to choose an auditing strategy that will monitor the primary and secondary databases so that it can maintain compliance in the event of a failover. Which choice does Microsoft recomend to meet this auditing requirement? A. Enable database-level auditing on the primary database. B. C. Enable both server-level and database-level auditing . Enable server-level auditing on the primary and secondary databases. . Enable database-level auditing on the primary and secondary databases.<>No options available<>No explanation provided.|||Q135<>You have an Azure Storage account and a data warehouse in Azure Synapse Analytics in the UK South region. You need to copy blob data from the storage account to the data warehouse by using Azure Data Factory. The solution must meet the following requirements: c® Ensure that the data remains in the UK South region at all times. ©® Minimize administrative effort. Which type of integration runtime should you use? A. Azure integration runtime B. Azure-SSIS integration runtime C. Self-hosted integration runtime<>No options available<>No explanation provided.|||Q136<>You have an Azure SQL database named Database! and two Azure event hubs named HubA and HubB. The data consumed from each source is shown in the following table. You need to implement Azure Stream Analytics to calculate the average fare per mile by driver. How should you configure the Stream Analytics input for each source? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Source Data Database1 Driver's name Driver's license number HubA Ride route Ride distance Ride duration HubB Ride fare Ride payment Answer Area HubA: Vv Stream Reference HubB: v Stream Reference Database: Stream |<>No options available<>No explanation provided.|||Q137<>You have an Azure Stream Analytics job that receives clickstream data from an Azure event hub. You need to define a query in the Stream Analytics job. The query must meet the following requirements: c® Count the number of clicks within each 10-second window based on the country of a visitor. c® Ensure that each click is NOT counted more than once. How should you define the Query? A. SELECT Country, Avg(*) AS Average FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, SlidingWindow(second, 10) B. SELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, TumblingWindow(second, 10) C. SELECT Country, Avg(*) AS Average FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, HoppingWindow(second, 10, 2) D. SELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, SessionWindow(second, 5, 10)<>No options available<>No explanation provided.|||Q138<>You are building an Azure Analytics query that will receive input data from Azure loT Hub and write the results to Azure Blob storage. You need to calculate the difference in the number of readings per sensor per hour. How should you complete the query? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area SELECT sensorId, growth = reading - WW) (reading) OVER (PARTITION BY sensorId iW] (hour, 1)) OFFSET WHEN FROM input<>No options available<>No explanation provided.|||Q139<>You need to schedule an Azure Data Factory pipeline to execute when a new file arrives in an Azure Data Lake Storage Gen2 container. Which type of trigger should you use? A. on-demand B. tumbling window C. schedule D. event<>No options available<>No explanation provided.|||Q140<>You have two Azure Data Factory instances named ADFdev and ADFprod. ADFdev connects to an Azure DevOps Git repository. You publish changes from the main branch of the Git repository to ADFdev. You need to deploy the artifacts from ADFdev to ADFprod. What should you do first? A. From ADFdev, modify the Git configuration. B. From ADFdev, create a linked service. C. From Azure DevOps, create a release pipeline. D. From Azure DevOps, update the main branch.<>No options available<>No explanation provided.|||Q141<>You are developing a solution that will stream to Azure Stream Analytics. The solution will have both streaming data and reference data. Which input type should you use for the reference data? A. Azure Cosmos DB B. Azure Blob storage C. Azure loT Hub D. Azure Event Hubs<>No options available<>No explanation provided.|||Q142<>You are designing an Azure Stream Analytics job to process incoming events from sensors in retail environments. You need to process the events to produce a running average of shopper counts during the previous 15 minutes, calculated at five-minute intervals. Which type of window should you use? A. snapshot B. tumbling C. hopping D. sliding<>No options available<>No explanation provided.|||Q143<>You are designing a monitoring solution for a fleet of 500 vehicles. Each vehicle has a GPS tracking device that sends data to an Azure event hub once per minute. You have a CSV file in an Azure Data Lake Storage Gen2 container. The file maintains the expected geographical area in which each vehicle should be. You need to ensure that when a GPS position is outside the expected area, a message is added to another event hub for processing within 30 seconds. The solution must minimize cost. What should you include in the solution? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Service: iv ‘An Azure Synapse Analytics Apache Spark pool An Azure Synapse Analytics serverless SQL pool Azure Data Factory Azure Stream Analytics Window: v Hopping No window Session Tumbling<>No options available<>No explanation provided.|||Q144<>You are designing an Azure Databricks table. The table will ingest an average of 20 million streaming events per day. You need to persist the events in the table for use in incremental load pipeline jobs in Azure Databricks. The solution must minimize storage costs and incremental load times. What should you include in the solution? A. Partition by DateTime fields. B. Sink to Azure Queue storage. C. Include a watermark column. D. Use a JSON format for physical data storage.<>No options available<>No explanation provided.|||Q145<>You have a self-hosted integration runtime in Azure Data Factory. The current status of the integration runtime has the following configurations: c® Status: Running c Type: Self-Hosted ‘Answer Area ce Version: 4.4.7292.1 > Running / Registered Node(s): 1/1 c® High Availability Enabled: False ce Linked Count: 0 © Queue Length: 0 c> Average Queue Duration. 0.00s If the X-M node becomes unavailable, all executed pipelines will: The integration runtime has the following node details: The number of concurrent jobs and the c> Name: X-M CPU usage indicate that the Concurrent c® Status: Running Jobs (Running/Limit) value should be: ce Version: 4.4.7292.1 c® Available Memory: 7697MB © CPU Utilization: 6% c> Network (In/Out): 1.21KBps/0.83KBps c® Concurrent Jobs (Running/Limit): 2/14 Dispatcher/Worker al Status: v fail until the node comes back online switch to another integration runtime exceed the CPU limit iv raised lowered left as is<>No options available<>No explanation provided.|||Q146<>You have an Azure Databricks workspace named workspace in the Standard pricing tier. You need to configure workspace] to support autoscaling all-purpose clusters. The solution must meet the following requirements: © Automatically scale down workers when the cluster is underutilized for three minutes. ©® Minimize the time it takes to scale to the maximum number of workers. © Minimize costs. What should you do first? A. Enable container services for workspace1. B. Upgrade workspace! to the Premium pricing tier. C. Set Cluster Mode to High Concurrency. D. Create a cluster policy in workspace.<>No options available<>No explanation provided.|||Q147<>You are designing an Azure Stream Analytics solution that will analyze Twitter data. You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once. Solution: You use a tumbling window, and you set the window size to 10 seconds. Does this meet the goal? A. Yes B. No Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. The following diagram illustrates a stream with a series of events and how they are mapped into 10- second tumbling windows. Tell me the count of tweets per time zone every 10 seconds F 2:34:59 / 3:02:47<>No options available<>No explanation provided.|||Q148<>You are designing an Azure Stream Analytics solution that will analyze Twitter data. You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once. Solution: You use a session window that uses a timeout size of 10 seconds. Does this meet the goal? A. Yes B. No<>No options available<>No explanation provided.|||Q149<>You use Azure Stream Analytics to receive data from Azure Event Hubs and to output the data to an Azure Blob Storage account. You need to output the count of records received from the last five minutes every minute. Which windowing function should you use? A. Session B. Tumbling C. Sliding D. Hopping Hopping window functions hop forward in time by a fixed period. It may be easy to think of them as Tumbling windows that can overlap and be emitted more often than the window size. Events can belong to more than one Hopping window result set. To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size. Every 5 seconds give me the YE count of Tweets over the last 10 seconds oo > Pl @  23659/3:02:47<>No options available<>No explanation provided.|||Q150<>You configure version control for an Azure Data Factory t ¢ instance as shown in the following exhibit. Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic. NOTE: Each correct selection is worth one point. Answer Area ‘Azure Resource Manager (ARM) templates for the pipeline assets are stored in [answer choice] B Unked services {© Integration runtimes @  scurcecontt By © Steontoaton © AaM template [@ Parameterization template tor $ ones Global parameters secu © customer managed ey Git repository Gi repository ntrmationaszocated with your data factory CVCD best practices [ ® senting Disconnect Repository type ‘Azure DevOps Git ‘Azure DevOps Account CONTOSO Project name ata Repository name duh bateheth Collaboration branch main Publish branch adt publish Root folder 1 if main adf_publish Parameterization template<>No options available<>No explanation provided.|||Q151<>You are designing an Azure Stream Analytics solution that receives instant messaging data from an Azure Event Hub. You need to ensure that the output from the Stream Analytics job counts the number of messages per time zone every 15 seconds. How should you complete the Stream Analytics query? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Select TimeZone, count (*) AS MessageCount FROM MessageStream V_ | createdat last OVER ‘SYSTEM.TIMESTAMP() TIMESTAMP BY GROUP BY TimeZone, (second, 15) HOPPINGWINDOW <0 @<>No options available<>No explanation provided.|||Q152<>You have an Azure Data Factory instance named ADF1 and two Azure Synapse Analytics workspaces named WS1 and WS2. ADF1 contains the following pipelines: c® P1: Uses a copy activity to copy data from a nonpartitioned table in a dedicated SQL pool of WS1 to an Azure Data Lake Storage Gen2 account -® P2: Uses a copy activity to copy data from text-delimited files in an Azure Data Lake Storage Gen2 account to a nonpartitioned table in a dedicated SQL pool of WS2 You need to configure P1 and P2 to maximize parallelism and performance. Which dataset settings should you configure for the copy activity if each pipeline? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Pa: Vv Set the Copy method to Bulk insert Set the Copy method to PolyBase Set the Isolation level to Repeatable read Set the Partition option to Dynamic range P2: Vv Set the Copy method to Bulk insert Set the Copy method to PolyBase Set the Isolation level to Repeatable read > Plo @  2:39:5973:02:47<>No options available<>No explanation provided.|||Q153<>You have an Azure Storage account that generates 200,000 new files daily. The file names have a format of {YYYY}/{MM}/{DD}/{HH}/{CustomerID}.csv. You need to design an Azure Data Factory solution that will load new data from the storage account to an Azure Data Lake once hourly. The solution must minimize load times and costs. How should you configure the solution? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Load methodology: Full Load Incremental Load Load individual files as they arrive Trigger: Fixed schedule<>No options available<>No explanation provided.|||Q154<>You have the following Azure Data Factory pipelines: c® Ingest Data from System1 > Ingest Data from System2 co Populate Dimensions c® Populate Facts Ingest Data from System1 and Ingest Data from System2 have no dependencies. Populate Dimensions must execute after Ingest Data from System] and Ingest Data from System2. Populate Facts must execute after Populate Dimensions pipeline. All the pipelines must execute every eight hours. What should you do to schedule the pipelines for execution? A. Add an event trigger to all four pipelines. B. Add a schedule trigger to all four pipelines. C. Create a patient pipeline that contains the four pipelines and use a schedule trigger. D. Create a patient pipeline that contains the four pipelines and use an event trigger.<>No options available<>No explanation provided.|||Q155<>You are responsible for providing access to an Azure Data Lake Storage Gen2 account. Your user account has contributor access to the storage account, and you have the application ID and access key. You plan to use PolyBase to load data into an enterprise data warehouse in Azure Synapse Analytics. You need to configure PolyBase to connect the data warehouse to storage account. Which three components should you create in sequence? To answer, move the appropriate components from the list of components to the answer area and arrange them in the correct order. ‘Components ‘Answer Area a database scoped credential an asymmetric key =r @ SS a database encryption key an external file format<>No options available<>No explanation provided.|||Q156<>You are monitoring an Azure Stream Analytics job by using metrics in Azure. You discover that during the last 12 hours, the average watermark delay is consistently greater than the configured late arrival tolerance. What is a possible cause of this behavior? A. Events whose application timestamp is earlier than their arrival time by more than five minutes arrive as inputs. B. There are errors in the input data. C. The late arrival policy causes events to be dropped. D. The job lacks the resources to process the volume of incoming data.<>No options available<>No explanation provided.|||Q157<>You are building an Azure Stream Analytics job to retrieve game data. You need to ensure that the job returns the highest scoring record for each five-minute time interval of each game. How should you complete the Stream Analytics query? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area SELECT as HighestScore Collect(Score) CollectTop(1) OVER(ORDER BY Score Desc) Game, MAX(Score) TopOne() OVER(PARTITION BY Game ORDER BY Score Desc) FROM input TIMESTAMP BY CreatedAt GROUP BY lv Game Hopping(minute,5) Tumbling(minute,5) Dl) 2:45:59 73:02:47<>No options available<>No explanation provided.|||Q158<>You have an Azure Data Lake Storage account that contains a staging zone. You need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics. Solution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. Does this meet the goal?<>No options available<>No explanation provided.|||Q159<>You plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads: c® A workload for data engineers who will use Python and SQL. ® A workload for jobs that will run notebooks that use Python, Scala, and SQL. © A workload that data scientists will use to perform ad hoc analysis in Scala and R. The enterprise architecture team at your company identifies the following standards for Databricks environments: c® The data engineers must share a cluster. ® The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster. > All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists. You need to create the Databricks clusters for the workloads. Solution: You create a High Concurrency cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs. Does this meet the goal?<>No options available<>No explanation provided.|||Q160<>You are designing an Azure Databricks cluster that runs user-defined local processes. You need to recommend a cluster configuration that meets the following requirements: c® Minimize query latency. > Maximize the number of users that can run queries on the cluster at the same time. co Reduce overall costs without compromising other requirements. Which cluster type should you recommend? A. Standard with Auto Termination B. High Concurrency with Autoscaling C. High Concurrency with Auto Termination D. Standard with Autoscaling<>No options available<>No explanation provided.|||Q161<>You are building an Azure Data Factory solution to process data received from Azure Event Hubs, and then ingested into an Azure Data Lake Storage Gen2 container. The data will be ingested every five minutes from devices into JSON files. The files have the following naming pattern. /{deviceType}/in/{YYYY}/{MM}/{DD}/{HH}/{devicelD}_{YYYY}{MM}{DD}HH}{mm}.json You need to prepare the data for batch data processing so that there is one dataset per hour per deviceType. The solution must minimize read times. How should you configure the sink for the copy activity? To answer, select the appropriate options in the answer area. NOTE: Each correct selection is worth one point. Answer Area Parameter: v @pipeline(),TriggerTime @pipeline(),TriggerType @trigger().outputs.windowStartTime @trigger().startTime Naming pattern: Vv HdevicelD}/out/{YYYY}/{MM}/{0D}/{HH}.json HYYYY}/{MM)/{DD}/{deviceType}.json JNYYYV{MM}A{DD}/{HH}.json | AYYYY}/{MM}/{DD}/{HH}_{deviceType}.json > Pl @  2:49:5973:02:47<>No options available<>No explanation provided.|||Q162<>You are designing an Azure Data Lake Storage Gen2 structure for telemetry data from 25 million devices distributed across seven key geographical regions. Each minute, the devices will send a JSON payload of metrics to Azure Event Hubs. You need to recommend a folder structure for the data. The solution must meet the following requirements: c® Data engineers from each region must be able to build their own pipelines for the data of their respective region only. c® The data must be processed at least once every 15 minutes for inclusion in Azure Synapse Analytics serverless SQL pools. How should you recommend completing the structure? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content. NOTE: Each correct selection is worth one point. Values Answer Area (devicerD) / Value ral Value val Value {mmm) / (HH) / (DD) /{ MM) /(Y¥¥Y) .jsor {regionID}/{deviceID) {regionID} /raw (evyy) / (MM) /(DD}/ (HH)<>No options available<>No explanation provided.|||Q163<>You are implementing an Azure Stream Analytics solution to process event data from devices. The devices output events when there is a fault and emit a repeat of the event every five seconds until the fault is resolved. The devices output a heartbeat event every five seconds after a previous event if there are no faults present. A sample of the events is shown in the following table. DevicelD EventType EventTime 78cc5ht9-w357-684r- | HeartBeat 2020-12-01T19:00.000Z You need to calculate the uptime between the faults. w4fr-kr16h6p9874e How should you complete the Stream Analytics SQL ‘78ccSht9-w357-684r- | HeartBeat 2020-12-01T 19:05.000Z juery? To answer, select the appropriate options in the }4f-kri6h6p9874e query: ’ 78cc5ht9-w357-684r- | TemperatureSensorFault | 2020-12-01119:07.000Z answer area. w4fr-kr16h6p9874e NOTE: Each correct selection is worth one point. Answer Area SELECT DevicerD, MIN(EventTime) as StartTime, MAX(EventTime) as EndTime, DATEDIFF (second, MIN(EventTime), MAX (EventTime)) AS duration_in_seconds FROM input TIMESTAMP BY EventTime v (WHERE LAG(EventType, 1) OVER (LIMIT DURATION(second,5)) <> EventType WHERE IsFirst(second.S) = 1 GROUP BY DevicerD 159 / 3:02:47<>No options available<>No explanation provided.|||Q164<>You are creating a new notebook in Azure Databricks that will support R as the primary language but will also support Scala and SQL. Which switch should you use to switch between languages? A. %<language> B. @<Language > C. \\Islanguage >] D. \\(<language >)<>No options available<>No explanation provided.|||Q165<>You have an Azure Data Factory pipeline that performs an incremental load of source data to an Azure Data Lake Storage Gen2 account. Data to be loaded is identified by a column named LastUpdatedDate in the source table. You plan to execute the pipeline every four hours. You need to ensure that the pipeline execution meets the following requirements: ® Automatically retries the execution when the pipeline run fails due to concurrency or throttling limits. > Supports backfilling existing data in the table. Which type of trigger should you use? A. event B. on-demand C. schedule D. tumbling window<>No options available<>No explanation provided.|||Q166<>You are designing a solution that will copy Parquet files stored in an Azure Blob storage account to an Azure Data Lake Storage Gen2 account. The data will be loaded daily to the data lake and will use a folder structure of {Year}/{Month}/{Day}/. You need to design a daily Azure Data Factory data load to minimize the data transfer between the two accounts. Which two configurations should you include in the design? Each correct answer presents part of the solution. NOTE: Each correct selection is worth one point A. Specify a file naming pattern for the destination. B. Delete the files in the destination before loading the data. C. Filter by the last modified date of the source files. D. Delete the source files after they are copied.<>No options available<>No explanation provided.|||Q167<>You plan to build a structured streaming solution in Azure Databricks. The solution will count new events in five- minute intervals and report only events that arrive during the interval. The output will be sent to a Delta Lake table. Which output mode should you use? A. update B. complete C. append<>No options available<>No explanation provided.|||Q168<>You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1. You have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1. You plan to insert data from the files in container into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of Table1. You need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1. Solution: In an Azure Synapse Analytics pipeline, you use a data flow that contains a Derived Column transformation. Does this meet the goal?<>No options available<>No explanation provided.|||Q169<>A company has decided to implement an Azure Databricks solution. Two teams in the company have different requirements. Team 1 needs to do an exploratory analysis of data in an Azure Data Lake (Gen 2) hierarchical file system. Team 2 needs to run a data processing job twice a day. The job takes about one hour to run. Which types of clusters should be used to minimize costs? A. B. C. D. Team 1 should use an automated cluster, and Team 2 should use an interactive cluster. Team 1 should use an interactive cluster, and Team 2 should use an automated cluster. . Both teams should use an interactive cluster. . Both teams should use an automated cluster.<>No options available<>No explanation provided.|||Q170<>A dedicated SQL pool containing user information is accidentally dropped while it is in paused state. Upon investigation, you find that no user-defined restore point was created before the SQL pool was dropped. As a result of this accident, what statement regarding a snapshot of the dedicated SQL pool is correct? A. Azure Synapse Analytics creates a snapshot of the SQL pool with all restore points for the last 14 days. B. Azure Synapse Analytics creates a final snapshot automatically before drop action. C. . Azure Synapse Analytics creates an automatic snapshot with a default restore point of the date the SQL pool is created. D. Azure Synapse Analytics does not create a final snapshot.<>No options available<>No explanation provided.|||Q171<>Your company sends shared access signatures (SAS) to verified third parties for Azure storage access. To enhance security, you're considering additional options. Evaluate these security features. Which of these can be uniquely controlled through stored access policies? Limiting access to a specific IP address range Specifying when access via SAS token starts and ends Limiting access to specific Azure storage containers or objects Revoking issued SAS tokens POV><>No options available<>No explanation provided.